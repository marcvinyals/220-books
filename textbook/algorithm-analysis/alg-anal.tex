\chapter{What is Algorithm Analysis?}  
\label{CH:ALG:ANAL}
%
\newcommand{\illustr}[2]{\centerline{\psfig{figure=figs/#1,width=#2}}}
  
Algorithmic problems are of crucial importance in modern life. 
Loosely speaking, they are precisely formulated problems that can be solved in a
step-by-step, mechanical manner. 

\begin{Definition}[informal]
An \defnfont{algorithm} is a list of unambiguous rules that specify successive 
steps to solve a problem. A  \defnfont{computer program} is a clearly specified 
sequence of computer instructions implementing the algorithm. 
\end{Definition}

For example, a sufficiently detailed recipe for making a cake could
be thought of as an algorithm. Problems of this sort are not normally
considered as part of computer science. In this book, we deal with algorithms 
for problems of a more abstract nature. Important examples, all discussed in this 
book, and all with a huge number of practical applications, include: sorting a 
database, finding an entry in a database, finding a pattern in a text document, 
finding the shortest path through a network, scheduling tasks as efficiently as 
possible, finding the median of a statistical sample.

Strangely enough, it is very difficult to give simple precise mathematical
definitions of algorithms and programs. The existing very deep general
definitions are too complex for our purposes. We trust that the reader
of this book will obtain a good idea of what we mean by algorithm from
the examples in this and later chapters.

We often wish to compare different algorithms for the same problem,
in order to select the one best suited to our requirements. The main
features of interest are: whether the algorithm is \defnfont{correct}
(does it solve the problem for all legal inputs), and how \emph{efficient} it 
is (how much time, memory storage, or other resources it uses).

The same algorithm can be implemented by very different programs written
in different programming languages, by programmers of different levels of skill,
and then run on different computer platforms under different operating systems.
In searching for the best algorithm, general features of algorithms must be 
isolated from peculiarities of particular platforms and programs. 

To analyse computer algorithms in practice, it is usually sufficient to
first specify elementary operations of a ``typical'' computer and then
represent each algorithm as a sequence of those operations.

Most modern computers and languages build complex programs from
ordinary arithmetic and logical operations such as standard unary and
binary arithmetic operations (negation, addition, subtraction,
multiplication, division, modulo operation, or assignment), Boolean
operations, binary comparisons (``equals", ``less than", or
``greater than"), branching operations, and so on. It is quite natural
to use these basic computer instructions as algorithmic operations,
which we will call \defnfont{elementary operations}.

It is not always clear what should count as an elementary operation. For
example, addition of two 64-bit integers should definitely count
as elementary, since it can be done in a fixed time for a given
implementation. But for some applications, such as cryptography, we must
deal with much larger integers, which must be represented in another
way. Addition of ``big" integers takes a time roughly proportional to
the size of the integers, so it is not reasonable to consider it as
elementary. From now on we shall ignore such problems. For most of the
examples in this introductory book they do not arise. However, they must
be considered in some situations, and this should be borne in mind.

\begin{Definition} [informal]
The \defnfont{running time} (or computing time) of an algorithm
is the number of its elementary operations.
\end{Definition}

The actual execution time of a program implementing an algorithm is
roughly proportional to its running time, and the scaling factor depends
only on the particular implementation (computer, programming language,
operating system, and so on).

The memory space required for running an algorithm depends on how many individual
variables (input, intermediate, and output data) are involved
simultaneously at each computing step. Time and space requirements
are almost always independent of the programming language or style and
characterise the algorithm itself. From here on, we will measure
effectiveness of algorithms and programs mostly in terms of their time
requirements. Any real computer has limits on the size and the number of
data items it can handle.

\section{Efficiency of algorithms: first examples}
\label{sec:efficiency}

If the same problem can be solved by different algorithms, then all other
things being equal, the most efficient algorithm uses least computational
resources. The theory of algorithms in modern computer science clarifies
basic algorithmic notions such as provability, correctness, complexity,
randomness, or computability. It studies  whether there exist any
algorithms for solving certain problems, and if so, how fast can they
be. In this book, we take only a  few small steps into this domain.

To search for the most efficient algorithm, one should mathematically
prove correctness of and determine time/space resources for each
algorithm as explicit functions of the size of input data to process.
For simplicity of presentation in this book, we sometimes skip the first 
step (proof of correctness), although it is very important. The focus of this 
chapter is to introduce methods for estimating the resource requirements of 
algorithms.

\begin{Example}[Sum of elements of an array]
\label{ex:lin:sum} 
Let $a$ denote an array of integers where the sum
\(
s = \sum_{i=0}^{n-1} a[i]
\) is required.
To get the sum \(s\), we have to repeat \(n\) times  
the same elementary operations (fetching 
from memory and adding a number). Thus, running time $T(n)$
is proportional to, or \emph{linear} in $n$: 
\( T(n) = c  n \). Such
algorithms are also called \defnfont{linear algorithms}. The unknown  
factor $c$ depends on a particular computer, 
programming language, compiler, operating system, etc. But 
the relative change in running time is 
just the same as the change in the data size:  
$T(10) = 10  T(1)$, or $T(1000) = 1000  T(1)$, 
or $T(1000)=10 T(100)$. The linear 
algorithm in Figure~\ref{fig:alg-linsum} implements a simple loop.

\begin{figure}
\hspace*{1.2in}\begin{minipage}{5in}
\Algorithm{linearSum}{array $a[0..n-1]$}{the sum \(s\)}
{
\> \(s \leftarrow 0\)\\
\> \textbf{for} \(i \leftarrow  0\) \textbf{step} \(i \leftarrow i+1\)
                            \textbf{until} \(n-1\) \textbf{do}\\
\> \> \(s \leftarrow s + a[i] \) \\
\> \textbf{end for} \\
\> \textbf{return} $s$\\
}
\end{minipage}
\caption{Linear-time algorithm to sum an array.}
\label{fig:alg-linsum}
\end{figure}

\end{Example}

\begin{Example} [Sums of subarrays]
\label{ex:con:subseq} 
The problem is to compute, for each subarray
$a[j..j+m-1]$ of size \(m\) in an array
$a$ of size \(n\), the partial sum of its elements $s[j]= 
\sum_{k=0}^{m-1}a[j+k]$; \(j=0,\ldots,n-m\). The total number of these
subarrays is \(n-m+1\). At first glance, we need to compute
\(n-m+1\) sums, each  of $m$ items, so that the running time is
proportional to $m  (n-m+1)$. If $m$ is fixed, the time depends still
linearly on $n$.

But if \(m\) is growing with $n$ as
a fraction of \(n\), such as \(m=\frac{n}{2}\), 
then 
\(
T(n) = c  \frac{n}{2}  \left(\frac{n}{2}+1\right)\) \( = 
0.25cn^{2} + 0.5cn 
\). 
The relative weight of the linear part, \(0.5cn\), 
decreases quickly with respect to the quadratic one
as $n$ increases. For example, if \(T(n) = 0.25n^{2} + 0.5n\), we see in 
the last column of Table~\ref{table:quad-beats-lin} the rapid decrease of the 
ratio of the two terms.

\begin{table}[hbtp]
\caption{Relative growth of linear and quadratic terms in an expression.}
\label{table:quad-beats-lin}
\begin{center}
\begin{tabular}{|r|r|r|r|r|} \hline
$n$ & $T(n)$ & $0.25n^{2}$ & \multicolumn{2}{c|}{$0.5n$}\\ \cline{4-5}
    &        &             & \textbf{value} & \% of quadratic term \\ \hline
10  & 30     &  25  & 5  & 20.0\\
50  & 650    & 625  & 25 & 4.0\\
100 & 2550   & 2500 & 50 & 2.0\\ 
%\end{tabular}
%\begin{tabular}{|r|r|r|r|r|} \hline
%$n$ & $T(n)$ & $0.25n^{2}$ & \multicolumn{2}{c|}{$0.5n$}\\ \cline{4-5}
%    &        &             & \textbf{value} & \% \\ \hline
500 & 62750  & 62500 & 250 & 0.4\\
1000& 250500 &250000 & 500 & 0.2\\ 
5000& 6252500&6250000&2500 & 0.04\\\hline
\end{tabular}
\end{center}
\end{table}

Thus, for large $n$ only the quadratic term becomes important
and the running time is roughly proportional to $n^{2}$,
or is quadratic in $n$. Such algorithms are sometimes called
\defnfont{quadratic algorithms} in terms of relative changes of running time with
respect to changes of the data size: if \(T(n) \approx cn^2\) then $T(10)
\approx 100 T(1)$, or $T(100) \approx 10000 T(1)$, or $T(100) \approx
100 T(10)$.


\begin{figure}[htbp]
\begin{center}
\begin{minipage}{5in}
\Algorithm{slowSums}
{array $a[0..2m-1]$}
{array $s$ of size \(m+1\)}
{
\> array $s[0..m]$\\
\> \textbf{for} \(i \leftarrow  0\) \textbf{to} $m$ \textbf{do}\\
\> \>\(s[i] \leftarrow 0\)\\
\> \>\textbf{for} \(j \leftarrow  0\) \textbf{to} \(m-1\) \textbf{do}\\
\> \>\>\(s[i] \leftarrow s[i] + a[i+j] \) \\
\> \>\textbf{end for} \\
\> \textbf{end for} \\
\> \textbf{return} $s$ \\
}
\end{minipage}
\end{center}
\caption{Quadratic time algorithm to compute sums of an array.}
\label{fig:alg-bad-array-sum}
\end{figure}

The ``brute-force'' quadratic algorithm has two nested loops (see
Figure~\ref{fig:alg-bad-array-sum}).
Let us analyse it to find out whether it can be simplified. It is easily
seen that repeated computations in the innermost loop are unnecessary.
Two successive sums $s[i]$ and $s[i-1]$ differ only by
two elements: \(s[i]=s[i-1]+ a[i+m-1]-a[i-1]\). Thus we need not
repeatedly add $m$ items together after getting the very first sum
$s[0]$. Each next sum is formed from the current one by using only two
elementary operations (addition and subtraction). Thus $T(n) = c  (m + 2
 (n-m)) = c  (2n - m)$. In the first parentheses, the first term $m$
relates to computing the first sum $s[0]$, and the second term $2 
(n-m)$ reflects that $n-m$ other sums are computed with only two
operations per sum. Therefore, the running time for this better
organized computation is always linear in $n$ for each value $m$, either
fixed or growing with $n$. The time for computing all the sums of the
contiguous subsequences is less than twice that taken for the single sum of all 
$n$ items in Example~\ref{ex:lin:sum}
 
The linear algorithm in Figure~\ref{fig:alg-good-array-sum}
excludes the innermost loop of the quadratic
algorithm. Now two simple loops, doing $m$ and $2(n-m)$ elementary
operations, respectively, replace the previous nested loop performing $m
 (n-m+1)$ operations. 
\end{Example} 

\begin{figure}[hbtp]
\begin{center}
\begin{minipage}{5in}
\Algorithm{fastSums}
{array $a[0..2m-1]$}
{array $s$ of size \(m+1\)}
{
\> array $s[0..m]$\\
\> \(s[0] \leftarrow 0\) \\
\> \textbf{for} \(j \leftarrow  0\) \textbf{to} \(m-1\) \textbf{do}\\
\> \>\(s[0] \leftarrow s[0] + a[j] \) \\
\> \textbf{end for} \\
\> \textbf{for} \(i \leftarrow  1\) \textbf{to} \(m\) \textbf{do}\\
\> \>\(s[i] \leftarrow s[i-1] + a[i+m-1]- a[i-1] \) \\
\> \textbf{end for} \\
\> \textbf{return} $s$;\\
}
\end{minipage}
\end{center}
\caption{Linear-time algorithm to compute sums of an array.}
\label{fig:alg-good-array-sum}
\end{figure}

Such an outcome is typical for algorithm analysis. In many cases, a
careful analysis of the problem allows us to replace a straightforward
``brute-force'' solution with much more effective one. But there are no
``standard'' ways to reach this goal. To exclude unnecessary
computation, we have to perform a thorough investigation of the problem
and find hidden relationships between the input data and desired
outputs. In so doing, we should exploit  all the tools we have learnt.
This book presents many examples where analysis tools are indeed useful,
but knowing how to analyse and solve each particular problem is still close to
an art. The more examples and tools are mastered, the more the art is
learnt. 

\subsection*{Exercises}

\begin{Exercise}
\label{exr:time-compl:2}
A quadratic algorithm with processing time \(T(n)=cn^2 \)
uses 500 elementary operations for processing $10$ data items. How many will it 
use for processing $1000$ data items? 
\end{Exercise}


\begin{Exercise}
\label{exr:time-compl:7A}
Algorithms \textbf{A} and 
\textbf{B} use
exactly \(T_\mathrm{A}(n) = c_\mathrm{A} n \lg n\)
and \(T_\mathrm{B}(n) = c_\mathrm{B} n^{2}\) elementary operations, 
respectively, for a problem of size \(n\).
Find the fastest algorithm for processing \(n=2^{20}\) data items if
\textbf{A} and \textbf{B} spend 10 and 1 operations,
respectively, to process \(2^{10}\equiv 1024\) items.
\end{Exercise}

\section{Running time for loops and other computations}
\label{rt-loops}

The above examples show that running time depends considerably on how
deeply the loops are nested and how the loop control variables are
changing. Suppose the control variables change linearly in $n$, that
is, increase or decrease by constant steps. If the number of elementary
operations in the innermost loop is constant, the nested loops result
in polynomial running time $T(n) = c  n^{k}$ where $k$ is
the highest level of nesting and $c$ is some constant. The first three
values of $k$ have special names: \defnfont{linear time}, 
\defnfont{quadratic time},
and \defnfont{cubic time} for $k=1$ (a single loop), $k=2$ (two nested
loops), and $k=3$ (three nested loops), respectively.

When loop control variables change non-linearly,
the running time also varies non-linearly with $n$.

\begin{Example} 
An exponential change \(i=1,k,k^2 ,\ldots, k^{m-1}\) of
the control variable  in the
range \(1 \le i \le n\) results in \defnfont{logarithmic time}
for a simple loop. The loop executes $m$ 
iterations such that $k^{m-1} \le  n  < k^{m}$. Thus, $m-1 \le \log_{k} n
< m$, and $T(n) = c  \lceil \log_{k} n \rceil$.
\end{Example}
 
Additional conditions for executing inner loops only for special values of 
the outer variables also decrease running time. 

\begin{Example} 
\label{exm:nest2}
Let us roughly estimate the running time of the following nested loops:
 
\hspace*{.3in}\begin{minipage}{5in}
\Algorbody
{
\(m \leftarrow 2\) \\
\textbf{for} \(j \leftarrow 1\) \textbf{to} \(n\) \textbf{do}\\
\>\textbf{if} \(j = m \) \textbf{then} \\
\>\> \(m \leftarrow 2m\) \\
\>\>\textbf{for} \(i \leftarrow  1\) \textbf{to} \(n\) \textbf{do}\\
\>\>\>$\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\textbf{end for} \\
\>\textbf{end if}\\
\textbf{end for}\\
}
\end{minipage}

The inner loop is executed $k$ times for $j=2, 4, \ldots, 2^{k}$
where $k < \lg n \le k+1$. The total time for the elementary operations is 
proportional to $kn$, that is, $T(n)=  n  \lfloor \lg n \rfloor$.    
\end{Example}



Conditional and switch operations like \textbf{if}
\{\emph{condition}\} \textbf{then} \{\emph{constant running time}
$T_1$\} \textbf{else} \{\emph{constant running time} $T_2$\} involve
relative frequencies of the groups of computations.  The running time
$T$ satisfies $T = f_\mathrm{true} T_1 + (1 - f_\mathrm{true}) T_2 <
\max \{ T_1, T_2 \}$ where $f_\mathrm{true}$ is the relative frequency
of the true condition value in the if-statement.

The running time of a function or method call is $T = \sum_{i=1}^{k}T_i$
where $T_i$ is the running time of statement \(i\) of the function
and $k$ is the number of statements.


\subsection*{Exercises}

\begin{Exercise}
\label{exm:nest1}
Is the running time quadratic or linear for the nested loops below?\\

\begin{minipage}{5in}
\Algorbody
{
\(m \leftarrow 1\) \\
\textbf{for} \(j \leftarrow 1\) \textbf{step} \(j \leftarrow j+1\) 
                                    \textbf{until} \(n\) \textbf{do}\\
\>\textbf{if} \(j = m \) \textbf{then} \(m \leftarrow m \cdot (n-1)\) \\
\>\>\textbf{for} \(i \leftarrow  0\) \textbf{step} \(i \leftarrow i+1\) 
                                       \textbf{until} \(n-1\) \textbf{do}\\
\>\>\>$\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\textbf{end for} \\
\>\textbf{end if}\\
\textbf{end for} \\
}
\end{minipage}

\end{Exercise}

\begin{Exercise}
\label{ex:nest2}

What is the running time for the following code fragment as a function of $n$?
\begin{center}
\begin{minipage}{5in}
\Algorbody
{
\textbf{for} $i \gets 1$ \textbf{step} $i \gets 2*i$ \textbf{while} $i<n$ \textbf{do} \\
\> \textbf{for} $j \gets 1$ \textbf{step} $j \gets 2*j$ \textbf{while} $j<n$ \textbf{do} \\
\> \> \textbf{if} $j = 2*i$ \\
\> \> \> \textbf{for} $k = 0$ \textbf{step} $k\gets k+1$ \textbf{while} $k<n$ \textbf{do} \\
\> \> \> \> $\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\> \textbf{end for}\\
\> \> \textbf{else} \\
\> \> \> \textbf{for} $k \gets 1$ \textbf{step} $k\gets 3*k$ \textbf{while} $k<n$ \textbf{do} \\
\> \> \> \> $\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\> \textbf{end for}\\
\>\> \textbf{end if} \\
\> \textbf{end for} \\
\textbf{end for}
}
\end{minipage}
\end{center}
\end{Exercise}

\section{``Big-Oh'', ``Big-Theta'', and ``Big-Omega'' tools} \label{tools}

Two simple concepts separate properties of an algorithm itself from
properties of a particular computer, operating system, programming
language, and compiler used for its implementation. The concepts,
briefly outlined earlier, are as follows: 
\begin{itemize}
 \item The \defnfont{input data size}, or
the number $n$ of individual data items in a single data instance to be
processed when solving a given problem. Obviously, how to measure the data
size depends on the problem: \(n\) means the number of items to sort (in sorting
applications),
number of nodes (vertices) or arcs (edges) in graph algorithms, number
of picture elements (pixels) in image processing, length of a character
string in text processing, and so on.  
\item The number of elementary operations taken by a particular algorithm, or its
running time. We assume it is a function $f(n)$ of the input data size
$n$. The function depends on the elementary operations chosen to build the
algorithm. 
\end{itemize} 

The running time of a program which implements
the algorithm is $c  f(n)$ where $c$ is a constant factor depending
on a computer, language, operating system, and compiler. Even if we don't know 
the value of the factor \(c\), we are able to answer the important question:
\emph{if the input size increases from $n=n_{1}$ to $n=n_{2}$, how does
the relative running time of the program change, all other things being equal?}
The answer is obvious: the 
running time increases by a factor of 
\( 
\frac{T(n_{2})}{T(n_{1})} = 
\frac{c  f(n_{2})}{c  f(n_{1})} =  
\frac{f(n_{2})}{f(n_{1})}
\). 

As we have already seen, the approximate running time for large input
sizes gives enough information to distinguish between a good and a bad
algorithm. Also, the constant $c$ above can rarely be determined. We
need some mathematical notation to avoid having to say ``\emph{of the
order of} $\ldots$''  or ``\emph{roughly proportional to} $\ldots$'',
and to make this intuition precise.

The standard mathematical tools ``\emph{Big Oh}'' ($O$), ``\emph{Big
Theta}'' ($\Theta$), and ``\emph{Big Omega}'' ($\Omega$) do 
precisely this.

\begin{note}
Actually, the above letter $O$ is a capital ``omicron''  (all
letters in this notation are Greek letters). However, since the Greek
omicron and the English ``O'' are indistinguishable in most fonts, we
read $O()$ as ``Big Oh'' rather than ``Big Omicron''. 
\end{note}

The algorithms are analysed under the following assumption: \emph{if
the running time of an algorithm as a function of $n$ differs only by a
constant factor from the  running time for another algorithm, then the
two algorithms have essentially the same running time.} Functions that
measure running time, $T(n)$, have nonnegative values because time is
nonnegative, $T(n) \ge 0$. The integer argument $n$ (data size) is also
nonnegative.

\begin{Definition} [Big Oh]
\label{def:oh}
Let $f(n)$ and $g(n)$ be nonnegative-valued functions defined on
nonnegative integers $n$. Then $g(n)$ is $O(f(n))$ (read ``\(g(n)\) is
Big Oh of \(f(n)\)'') iff there exists 
a positive real constant $c$ and
a positive integer $n_{0}$ such that $g(n) \le c f(n)$ for all $n>n_{0}$.
\end{Definition}
\begin{note} 
We use the notation ``\defnfont{iff}'' as an abbreviation of ``if and only if''.
\end{note}
In other words, if $g(n)$ is $O(f(n))$ then an algorithm with running time 
$g(n)$ runs for large $n$ at least as fast, to within a
constant factor, as an algorithm with running time  $f(n)$. Usually
the term ``\defnfont{asymptotically}'' is used in this context to describe 
behaviour of functions for sufficiently large values of $n$. This term 
means that \(g(n)\) for large \(n\) may approach closer and closer to 
\(c\cdot f(n)\). Thus, \(O(f(n))\) specifies
an \defnfont{asymptotic upper bound}.
\begin{note} 
Sometimes the ``Big Oh'' property is denoted $g(n)=O(f(n))$, but
we should not assume that the function $g(n)$ is equal to something 
called ``Big Oh'' of $f(n)$. This notation really means $g(n) \in O(f(n))$, 
that is, \(g(n)\) is a member of the set $O(f(n))$ 
of functions which are increasing, in essence, with the same 
or lesser rate as  \(n\) tends to infinity ($n \rightarrow \infty$). 
In terms of graphs of these functions, $g(n)$ is $O(f(n))$ iff
there exists a constant $c$ such that the graph of $g(n)$ is always
below or at the graph of $cf(n)$ after a certain point, $n_{0}$. 
\end{note}

\begin{Example}
Function \(g(n)=100\log_{10}n\) in Figure~\ref{f:aa-graphs} 
is \(O(n)\) because the graph \(g(n)\) is always below
the graph of \(f(n)=n\) if \(n > 238\) or of \(f(n)=0.3n\)
if \(n > 1000\), etc. 
\end{Example}

\begin{figure}[htb!]
\centerline{
\illustr{aa-graphs.eps}{80mm}}
\caption{``Big Oh'' property: $g(n)$ is $O(n)$.}
\label{f:aa-graphs}
\end{figure}
\begin{Definition} [Big Omega]
\label{def:omega}
The function $g(n)$ is $\Omega(f(n))$ iff 
there exists a positive real constant $c$ and a 
positive integer $n_{0}$  
such that $g(n) \ge c f(n)$ for all $n>n_{0}$.
\end{Definition} 

``Big Omega'' is complementary to ``Big Oh'' and generalises the
concept of ``lower bound'' (\(\ge\)) in the same way as ``Big Oh''
generalises the concept of ``upper bound'' (\(\le\)): if
\(g(n)\) is \(O(f(n))\) then \(f(n)\) is \(\Omega(g(n))\), and vice versa. 

\begin{Definition} [Big Theta]
\label{def:theta}
The function $g(n)$ is $\Theta(f(n))$ iff 
there exist two positive real constants $c_1$ and $c_2$ and a 
positive integer $n_{0}$  
such that $c_1f(n) \le g(n) \le c_2 f(n)$ for all $n>n_{0}$.
\end{Definition}

Whenever two functions, \(f(n)\) and \(g(n)\), are actually of the same
order, \(g(n)\) is \(\Theta(f(n))\), they are each ``Big Oh'' of the
other: $f(n)$ is $O(g(n))$ and $g(n)$ is  $O(f(n))$. In other words,
\(f(n)\) is both an asymptotic upper and lower bound for \(g(n)\). The
``Big Theta'' property means \(f(n)\) and \(g(n)\) have asymptotically
tight bounds and are in some sense equivalent for our purposes.

In line with the above definitions,
\(g(n)\) is \(O(f(n))\) iff \(g(n)\) grows \boldfont{at 
most} as fast as \(f(n)\) to within a constant factor, \(g(n)\) is
\(\Omega(f(n))\) iff \(g(n)\) grows \boldfont{at 
least} as fast as \(f(n)\) to within a constant factor, and
\(g(n)\) is \(\Theta(f(n))\) iff \(g(n)\) and \(f(n)\) grow
\boldfont{at the same rate} to within a constant factor.


``Big Oh'', ``Big Theta'', and ``Big Omega'' notation formally capture 
two crucial ideas in comparing algorithms:
the exact function, $g$, is not very
important because it can be multiplied by any
arbitrary positive constant, $c$, and
the relative behaviour of two functions
is compared only asymptotically, for large $n$, but not near the
origin where it may make no sense.
Of course, if the constants involved are very large, 
the asymptotic behaviour loses practical interest. In most
cases, however, the constants remain fairly small. 

In analysing running time,
``Big Oh'' $g(n) \in O(f(n))$, ``Big Omega'' $g(n) \in \Omega(f(n))$,
and ``Big Theta'' $g(n) \in \Theta(f(n))$  
definitions are mostly used with $g(n)$ equal to 
``exact'' running time on inputs of size $n$ and 
$f(n)$ equal to a rough approximation to
running time (like \(\log n\), \(n\), \(n^2\), and so on).
 
To prove that some
function \(g(n)\) is \(O(f(n))\), \(\Omega(f(n))\), or 
\(\Theta(f(n))\) using the definitions we need to find the constants \(c\), \(n_0\) or \(c_1\), \(c_2\), \(n_0\) specified in 
Definitions~\ref{def:oh}, \ref{def:omega}, \ref{def:theta}. 
Sometimes the proof is given only by a chain of inequalities,
starting with \(f(n)\). In other cases it
may involve more intricate techniques, such as mathematical
induction. Usually the manipulations
are quite simple. To prove that \(g(n)\) is \boldfont{not} 
\(O(f(n))\), \(\Omega(f(n))\), or \(\Theta(f(n))\) we have 
to show the desired constants do not exist, that is,
their assumed existence leads to a contradiction.

\begin{Example}
To prove that 
linear function $g(n) = an + b$; $a > 0$, is $O(n)$, we
form the following chain of inequalities:  
\(g(n) \le an + |b| \le (a+|b|)n\) for all \(n \ge 1\).
Thus, Definition~\ref{def:oh}
with \(c=a+|b|\) and \(n_0 = 1\) shows 
that \(an + b\) is \(O(n)\). 
\end{Example} 
 
``Big Oh'' hides constant factors so that both $10^{-10}n$ and
$10^{10}n$ are $O(n)$. It is pointless to write something like  $O(2 
n)$ or $O(a  n + b)$ because this still means $O(n)$. Also, only 
the dominant terms as  $n \rightarrow
\infty$ need be shown as the argument of ``Big Oh'', ``Big Omega'', or
``Big Theta''.

\begin{Example} 
    The polynomial $P_{5}(n) = a_{5}n^{5} + a_{4}n^{4}+a_{3}n^{3} 
+a_{2}n^{2}+a_{1}n +a_{0}$; $a_{5}>0$, is $O(n^{5})$ 
because  
\(P_{5}(n) \leq (a_{5}+|a_{4}|+|a_{3}|+|a_{2}|+|a_{1}|+|a_{0}|)n^{5}\)
for  all \(n \ge 1\). 
\end{Example}


\begin{Example} 
\label{ex:expons}
The exponential function $g(n) = 2^{n+k}$, where $k$ is a constant,
is $O(2^{n})$ because 
\(2^{n+k} = 2^{k}  2^{n}\) for all \(n\).
Generally, $m^{n+k}$ is $O(l^{n})$; $l \ge m > 1$, because 
$m^{n+k} \le l^{n+k} = l^{k}  l^{n}$ for any constant $k$. 
\end{Example}

\begin{Example} 
\label{ex:logs}
For each $m>1$, the logarithmic function $g(n) = \log_{m}(n)$ has the same rate of 
increase as $\lg(n)$ because 
\(\log_{m}(n) = \log_{m}(2)  \lg(n)\) for  all \(n > 0\).
Therefore we may omit the logarithm base when using 
the ``Big-Oh'' and ``Big Theta'' notation: \(\log_{m}n\) is \(\Theta(\log n)\).
\end{Example}


\subsection{Rules for asymptotic notation}
\label{o-features}
 
Using the definition to prove asymptotic relationships between functions is hard
work. As in calculus, where we soon learn to use various rules (product rule, 
chain rule, \dots) rather than the definition of derivative, we can use some 
simple rules to deduce new relationships from old ones.

We present rules for ``Big Oh"---similar relationships hold for ``Big Omega'' 
and ``Big Theta''.

We will consider the features both informally and formally using the
following notation. Let $x$ and $y$ be functions of a nonnegative
integer $n$. Then $z=x+y$ and $z=xy$ denote the sum of the functions,
$z(n) = x(n)+y(n)$, and the product function: $z(n) = x(n)y(n)$,
respectively, for every value of $n$. The product function $(xy)(n)$
returns the product of the values of the functions at $n$ and has
nothing in common with the composition $x(y(n))$ of the two functions.

Basic arithmetic relationships for ``Big Oh''
follow from and can be easily proven with its definition. 

\begin{Lemma}[Scaling] \label{l:bigoh:1}
For all constants $c > 0$, $c  f$ is $O(f)$. In particular, $f$ is $O(f)$. 
\end{Lemma}

\begin{proof}
The relationship \(cf(n) \leq c f(n)\) obviously holds for all \(n\geq 0\).
\end{proof}

Constant factors are ignored, and only the powers and functions are
taken into account. It is this ignoring of constant factors that
motivates such a notation.

\begin{Lemma}[Transitivity] \label{l:bigoh:2}
If $h$ is $O(g)$ and $g$ is $O(f)$, then $h$ is $O(f)$.
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

Informally, if $h$ grows at most as quickly as $g$, which grows 
at most as quickly as $f$, then $h$ grows at most as quickly as $f$. 

\begin{Lemma}[Rule of sums] \label{l:bigoh:3}
%{\textsc{Rule of sums.}} 
If $g_{1}$ is $O(f_{1})$ and 
$g_{2}$ is $O(f_{2})$, then 
$g_{1}+g_{2}$ is $O(\max \{ f_{1},f_{2} \} )$. 
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

If $g$ is $O(f)$ and $h$ is $O(f)$, then is $g+h$ is $O(f)$. In particular, 
if $g$ is $O(f)$, then $g+f$ is $O(f)$. Informally, the growth rate of a sum 
is the growth rate of its fastest-growing term.
 
%\newpage 

\begin{Lemma}[Rule of products]\label{l:bigoh:4}
%{\textsc{Rule of products.}}
 If $g_{1}$ is $O(f_{1})$ and 
$g_{2}$ is $O(f_{2})$, then 
$g_{1}  g_{2}$ is $O(f_{1}  f_{2})$. 
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

In particular, if $g$ is $O(f)$, then $g  h$ is $O(f  h)$. Informally,
the product of upper bounds of functions gives an upper bound for the
product of the functions.

Using calculus we can obtain a nice time-saving rule.

\begin{Lemma}[Limit Rule] \label{l:bigoh:lim}
Suppose $\lim_{n\to \infty} f(n)/g(n)$ exists (may be $\infty$), and denote 
the limit by $L$. Then
\begin{itemize}
\item if $L=0$, then $f$ is $O(g)$ and $f$ is not $\Omega(g)$;
\item if $0 < L < \infty$ then $f$ is $\Theta(g)$;
\item if $L = \infty$ then $f$ is $\Omega(g)$ and $f$ is not $O(g)$.
\end{itemize}
\end{Lemma}

\begin{proof}
If $L = 0$ then from the definition of limit, in particular there is some 
$n_0$ such that $f(n)/g(n) \leq 1$ for all $n \geq n_0$. Thus $f(n) \leq g(n)$ 
for all such $n$, and $f(n)$ is $O(g(n))$ by definition. On the other 
hand, for each $c > 0$, it is not the case that $f(n) \geq c g(n)$ for all
$n$ past some threshold value $n_1$, so that $f(n)$ is not $\Omega(g(n))$.
The other two parts are proved in the analogous way.
\end{proof}

To compute the limit if it exists, the standard \emph{L'H\^{o}pital's rule}
of calculus is useful (see Section~\ref{sec:app:lhopital}).

More specific relations follow directly from the basic ones.

\begin{Example} 
\label{ex:powers}
Higher powers of $n$ grow more quickly than lower powers:
$n^{k}$ is $O(n^{l})$ if $0 \le k \le l$. This follows directly from the limit rule
since $n^k/n^l = n^{k-l}$ has limit $1$ if $k=l$ and $0$ if $k<l$.
\end{Example}

\begin{Example}  
The growth rate of a polynomial is given by the growth rate of its leading 
term (ignoring the leading coefficient by the scaling feature): 
if $P_{k}(n)$ is a polynomial of exact degree $k$ then $P_{k}(n)$ is $\Theta(n^{k})$. 
This follows easily from the limit rule as in the preceding example.
\end{Example}


\begin{Example}  
Exponential functions grow more quickly than powers: 
$n^{k}$ is $O(b^{n})$, for all $b>1$, $n>1$, and $k \ge 0$. 
The restrictions on $b$, $n$, and $k$ merely ensure that both 
functions are increasing. This result can be proved by induction or by using the 
limit-L'H\^{o}pital approach above.
\end{Example}

\begin{Example}  
Logarithmic functions grow more slowly than powers: 
$\log_{b} n$ is $O(n^{k})$ for all $b>1$, $k > 0$. 
This is the inverse of the preceding feature. Thus, 
as a result, $\log n$ is $O(n)$ and $n \log n$ is $O(n^{2})$.  
\end{Example} 

\subsection*{Exercises}

\begin{Exercise}\label{exr:aa:bigO:a}
Prove that $10n^3 - 5n + 15$ is not $O(n^{2})$.
\end{Exercise}
\begin{Exercise}\label{exr:aa:bigO:b}
Prove that $10n^3 - 5n + 15$ is $\Theta(n^{3})$.
\end{Exercise}
\begin{Exercise}\label{exr:aa:bigO:c}
Prove that $10n^3 - 5n + 15$ is not $\Omega(n^{4})$.
\end{Exercise}

\begin{Exercise}\label{exr:bigtheta}
Prove that $f(n)$ is $\Theta(g(n))$ if and only if both $f(n)$ is $O(g(n)$ and 
$f(n)$ is $\Omega(g(n))$.
\end{Exercise}

\begin{Exercise}
\label{exr:bigoh-order}
Using the definition, show that each function \(f(n)\) in 
Table~\ref{t:data-size} stands in ``Big-Oh'' relation to the preceding 
one, that is, \(n\) is \(O(n\log n)\), \(n\log n\) is \(O(n^{1.5})\),
and so forth. 

\end{Exercise}

\begin{Exercise}\label{exr:bigoh:features}
Prove Lemmas~\ref{l:bigoh:2}--\ref{l:bigoh:4}.
\end{Exercise}

\begin{Exercise}\label{exr:bigomega:sums}
Decide on how to reformulate the Rule of Sums (Lemma~\ref{l:bigoh:3})
for ``Big Omega'' and ``Big Theta'' notation.
\end{Exercise}

\begin{Exercise}\label{exr:bigomega:lem}
Reformulate and prove Lemmas~\ref{l:bigoh:1}--\ref{l:bigoh:4}
for ``Big Omega'' notation. 
\end{Exercise}

\section{running time of algorithms} 
\label{time-compl}

\begin{Definition} [Informal]
A function $f(n)$ such that the running time $T(n)$ of a given 
algorithm is $\Theta(f(n))$ measures the \defnfont{running time} 
of the algorithm.
\end{Definition}

An algorithm is called \defnfont{polynomial time} if its running time $T(n)$
is $O(n^{k})$ where $k$ is some fixed positive integer. A computational
problem is considered \defnfont{intractable} iff no deterministic
algorithm with polynomial running time exists for it. But many problems
are classed as intractable only because a polynomial solution is unknown,
and it is a very challenging task to find such a solution for one of them.
 
\begin{table}[htb] 
\caption[Relative growth of running time $T(n)$ when 
  the input size increases.]%
{\label{t:growth} Relative growth of running time $T(n)$ when 
  the input size increases from $n=8$ to $n=1024$ provided that $T(8)=1$.} 
  \centerline{
   \begin{tabular}{|c|c|cccc|c|} \hline 
   \multicolumn{2}{|c|}{\textbf{running time}} &  
   \multicolumn{4}{|c|}{\textbf{Input size $n$}} & \textbf{Time} $T(n)$
\\ \cline{1-6} 
   \emph{Function} & \emph{Notation} & \emph{8} & \emph{32} & \emph{128} & \emph{1024} &
\\ \hline 
   Constant     & $1$      & 1  & 1  &   1 &   1 & 1 \\ 
\hline 
   Logarithmic  & $\lg n$ & 1  & 1.67  & 2.67  & 3.33 & \( \lg n / 3 \)\\ 
\hline 
   Log-squared  & $\lg^{2} n$ & 1 & 2.78  & 5.44 &  11.1 & \( \lg^{2} n / 9 \) 
\\ \hline 
   Linear       & $n$ & 1 & 4 & 16 & 128 & \( n / 8 \) \\ 
\hline 
   ``$n\log n$''& $n \lg n$ & 1 & 6.67 & 37.3 & 427 & \(n \lg n / 24\)
\\ \hline 
   Quadratic    & $n^{2}$ & 1 & 16 & 256 & 16384 & \(n^{2} / 64\)
\\ \hline 
   Cubic        & $n^{3}$ & 1 & 64 & 4096 & 2097152 & \(n^{3} / 512\)
\\ \hline 
   Exponential  & $2^{n}$ & 1 & $2^{24}$ & $2^{120}$ & $2^{1016}$ & \(2^{n-8}\)
\\ \hline 
   \end{tabular}} 
 \end{table} 

Table~\ref{t:growth} shows how the running time \(T(n)\) of algorithms
having different running time, $f(n)$, grows relatively with the
increasing input size $n$.  running time functions are listed in
order such that $g$ is $O(f)$ if $g$ is above $f$: for example, the
linear function \(n\) is \(O(n\log n) \) and \(O(n^2 )\), etc. The 
asymptotic growth rate does not depend on the base of the logarithm, but the 
exact numbers in the table do --- we use $\log_{2} = \lg$ for simplicity.

\begin{table}[htbp] 
\caption[The largest data sizes $n$ that can be
processed by an algorithm.]%
{\label{t:data-size} The largest data sizes $n$ that can be
processed by an algorithm with running time $f(n)$
provided that $T(10)= 1$ minute.} 
\begin{center}
   \begin{tabular}{|c|r|r|r|r|r|r|} \hline
      & \multicolumn{6}{|c|}{\textbf{Length of time to run an algorithm}}\\
                \cline{2-7}
$f(n)$  & 1 minute & 1 hour & 1 day & 1 week & 1 year & 1 decade  \\
\hline
  $n$         & 10  & 600 & 14 400 & 100 800 & $5.26\times 10^{6}$  &
$5.26\times  10^{7}$  \\ \hline 
  $n\lg n$    & 10  & 250 & 3 997 & 23 100  & 883 895   & $7.64\times 10^6$
  \\ \hline 
  $n^{1.5}$    & 10  & 153 &  1 275 &   4 666 &    65 128 &
   302,409   \\ \hline 
  $n^{2}$      & 10  &  77 &    379 &   1 003 &     7 249 &
    22,932   \\ \hline 
  $n^{3}$      & 10  &  39 &    112 &     216 &       807 &
     1,738  \\ \hline 
  $2^{n}$      & 10  &  15 &     20 &      23 &        29 &
        32   \\ \hline
   \end{tabular}
   \end{center} 
 \end{table} 
 
Table~\ref{t:data-size} is even more expressive in showing
how the running time of an algorithm affects the size of problems the
algorithm can solve (we again use $\log_{2} = \lg$). A linear algorithm solving 
a problem of size $n=10$ in exactly one minute will process about $5.26$ million  
data items per year and 10 times more if we can wait a decade. 
But an exponential algorithm  with \(T(10)=1\) minute will deal
only with $29$ data items after a year of running and add only $3$
more items after a decade.  Suppose we have computers $10,000$ times
faster (this is approximately the ratio of a week to a minute). Then 
we can solve a problem $10,000$ times, $100$ times, or $21.5$ times 
larger than before if our algorithm is linear, quadratic, or cubic, 
respectively. But for exponential algorithms, our progress is much worse: 
we can add only $13$ more input values if $T(n)$ is $\Theta(2^n)$.

Therefore, if our algorithm has a constant, logarithmic, log-square, linear, 
or even ``$n \log n$'' running time we may be happy and start writing a 
program with no doubt that it will meet at least some practical demands. 
Of course, before taking the plunge, it is better to check whether the hidden 
constant $c$, giving the computation volume per data item, is sufficiently small
in our case. Unfortunately, order relations can be drastically
misleading: for instance, two linear functions $10^{-4}n$ and $10^{10}n$
are of the same order $O(n)$, but we should not claim an
algorithm with the latter running time as a big success.

Therefore, we should follow a simple rule: \emph{roughly estimate 
the computation volume per data item for the algorithms after comparing
their time complexities in a ``Big-Oh'' sense!} We may estimate the
computation volume simply by counting the number of elementary
operations per data item.

In any case we should be \boldfont{very} careful even with simple
quadratic or cubic algorithms, and especially with exponential
algorithms. If the running time is speeded up in Table~\ref{t:data-size} so that
it takes one \emph{second} per ten data items
in all the cases, then we will still wait about 12 \emph{days}
(\(2^{20}\equiv 1,048,576\) seconds) for processing only 30 items by the 
exponential algorithm. Estimate yourself whether it is
practical to wait until 40 items are processed.
 
In practice, quadratic and cubic algorithms cannot be used if the input
size exceeds tens of thousands or thousands of items, respectively,
and exponential algorithms should be avoided whenever possible unless we
always have to process data of very small size. Because even the most
ingenious programming cannot make an inefficient algorithm fast (we
would merely change the value of the hidden constant $c$ slightly, but
not the asymptotic order of the running time), it is better to spend
more time to search for efficient algorithms, even at the expense of a
less elegant software implementation, than to spend time writing a very
elegant implementation of an inefficient algorithm. 

\subsection{Worst-case and average-case performance}
\label{ss:worst-vs-avg}

We have introduced asymptotic notation in order to measure the running time of 
an algorithm. This is expressed in terms of elementary operations. ``Big Oh", 
``Big Omega" and ``Big Theta" notations allow us to state upper, lower
 and tight asymptotic bounds on running time that are independent of inputs and
implementation details. Thus we can classify algorithms by 
performance, and search for the ``best'' algorithms for solving a particular problem.  

However, we have so far neglected one important point. In general,
\emph{the running time varies not only according to the size of the
input, but the input itself}. The examples in Section~\ref{time-compl} 
were unusual
in that this was not the case. But later we shall see many examples
where it does occur. For example, some sorting algorithms take almost
no time if the input is already sorted in the desired order, but much
longer if it is not.

If we wish to compare two different algorithms for the same problem, it
will be very complicated to consider their performance on all possible
inputs. We need a simple measure of running time.

The two most common measures of an algorithm are the \defnfont{worst-case 
running time}, and the \defnfont{average-case running time}. 

The worst-case running time has several advantages.  If we can show,
for example, that our algorithm runs in time $O(n\log n)$ no matter
what input of size $n$ we consider, we can be confident that even if we
have an ``unlucky" input given to our program, it will not fail to run
fairly quickly. For so-called ``mission-critical" applications this is
an essential requirement. In addition, an upper bound on the worst-case
running time is usually fairly easy to find.

The main drawback of the worst-case running time as a measure is that it may be 
too pessimistic. The real running time might be much lower than
an ``upper bound'', the input data causing  the worst case may be
unlikely to be met in practice, and the constants $c$ and $n_{0}$ of the 
asymptotic notation  are unknown and may not be small.
There are many algorithms for which it is difficult to specify the worst-case 
input. But even if it is known, the inputs actually encountered in practice may 
lead to much lower running times. We
shall see later that the most widely used fast sorting
algorithm, quicksort, has worst-case quadratic
running time, $\Theta(n^{2})$, but its running time
for ``random" inputs encountered in practice is $\Theta(n \log n)$.

By contrast, the average-case running time is not as easy to define. The use of 
the word ``average" shows us that probability is involved. We need to specify a 
probability distribution on the inputs. Sometimes this is not too difficult. 
Often we can assume that every input of size $n$ is equally likely, and this 
makes the mathematical analysis easier. But sometimes an assumption of this sort
 may not reflect the inputs encountered in practice. Even if it does, the
average-case analysis may be a rather difficult mathematical challenge
requiring intricate and detailed arguments. And of course the worst-case 
complexity may be very bad even if the average case complexity is good, so there
 may be considerable risk involved in using the algorithm.

Whichever measure we adopt for a given algorithm, our goal is  to show
that its running time is $\Theta(f)$ for some function $f$ \boldfont{and}
there is no algorithm with running time $\Theta(g)$ for any function $g$
that grows more slowly than $f$ when $n \rightarrow \infty$. In this case
our algorithm is \defnfont{asymptotically optimal} for the given problem.

Proving that no other algorithm can be asymptotically better than
ours is usually a difficult matter: we must carefully construct a
formal mathematical model of a computer and derive a lower bound on the
complexity of every algorithm to solve the given problem. In this book
we will not pursue this topic much. If our analysis does show that an
upper bound for our algorithm matches the lower one for the problem,
then we need not try to invent a faster one.

 
\subsection*{Exercises}


\begin{Exercise}\label{exr:aa:data-size}
Add columns to Table~\ref{t:data-size} corresponding to
one century (10 decades) and one millennium (10 centuries).
\end{Exercise}

\begin{Exercise}\label{exr:aa:time-cmplx}
Add rows to Table~\ref{t:growth} for 
algorithms with running time \(f(n)=\lg\lg n\)
and \(f(n)=n^{2}\lg n\).
\end{Exercise}

\section{Basic recurrence relations}
\label{sec:recurrences}

As we will see later, a great many algorithms are
based on the following \defnfont{divide-and-conquer} principle:
\begin{itemize} 
\item divide a large problem into
smaller subproblems and recursively solve each subproblem, then 
\item combine solutions of the subproblems to
solve the original problem.
\end{itemize}
Running time of such algorithms is determined by
a \defnfont{recurrence relation} accounting for
the size and number of the subproblems and for the cost
of splitting the problem into them. The recursive 
relation defines a function ``in terms of itself'', that is, 
by an expression that involves the same function. The definition is not 
circular provided that the value at a natural number $n$ is defined in terms 
of values at smaller natural numbers, and the recursion terminates at some 
base case below which the function is not defined.

\begin{Example} [Fibonacci numbers] 
\label{ex:fibonacci}
These are defined by one of the most famous recurrence relations:
\(F(n) = F(n-1) + F(n-2)\); \(F(1)=1\), and \(F(2)=1\).
The last two equations are called the 
\defnfont{base of the recurrence} or
\defnfont{initial condition}. The recurrence relation
uniquely defines the function \(F(n)\)
at any number $n$ because any particular
value of the function is easily obtained by generating
all the preceding values until the desired term is produced,
for example, \(F(3)=F(2)+F(1)=2\); 
\(F(4)=F(3)+F(2)=3\), and so forth. Unfortunately, to
compute \(F(10000)\), we need to perform 9998 additions.
\end{Example}

\begin{Example} \label{ex:exponential}
One more recurrence relation is  
\(T(n) = 2  T(n-1) + 1\) with the base condition \(T(0) = 0\).
Here, $T(1) = 2\cdot 0 + 1 = 1$,
$T(2) = 2\cdot 1 +1 = 3$, $T(3) = 2\cdot 3 + 1 = 7$,
$T(4) = 2\cdot 7 + 1 = 15$,
and so on.
\end{Example}

\begin{note} 
A recurrence relation is sometimes simply called a {\defnfont{recurrence}}.
In engineering it is called a {\defnfont{difference equation}}.
\end{note}

We will frequently meet recurrences in algorithm
analysis. It is more convenient to have an explicit
expression, (or \defnfont{closed-form expression}) for the function 
in order to
compute it quickly for any argument
value \(n\) and to compare it with other functions. The closed-form
expression for $T(n)$, that is,
what is traditionally called a ``formula'', makes 
the growth of $T(n)$ as
a function of $n$ more apparent. The
process of deriving the explicit expression is called
``\emph{solving the recurrence relation}''.

Our consideration will be restricted to only the two
simplest techniques for solving recurrences:
(\textit{i}) guessing a solution from a sequence of values
\(T(0)\), \(T(1)\), \(T(2)\), \ldots, and
proving it by mathematical
induction (a ``bottom-up'' approach)
and (\textit{ii}) ``telescoping'' the
recurrence (a ``top-down'' approach). 
Both techniques allow us
to obtain closed forms of some important recurrences 
that describe performance of sort and search algorithms.
For instance, in Example~\ref{ex:exponential} we can simply guess 
the closed form expression $T(n)=2^{n}-1$
by inspecting the first few terms of the sequence
0, 1, 3, 7, 15 because $0=1-1$, $1 = 2 -1$, $3=4 -1$, $7=8 -1$,
and $15=16 -1$. But in
other cases these techniques may fail and more
powerful mathematical tools beyond the scope of this book, such as
using characteristic equations and generating functions, should be applied.

\subsection{Guessing to solve a recurrence}

There is no formal way to find a closed-form solution. But after we 
have guessed the solution, it may be proven to be correct by 
mathematical induction (see Section~\ref{sec:app:ind}).

\begin{Example}\label{ex:exp:recurrence}
For the recurrence \(T(n) = 2T(n-1)+1\) with
the base condition \(T(0) = 0\) in Example~\ref{ex:exponential}
we guessed
the closed-form relationship \(T(n) = 2^{n} - 1\) by
analysing the starting terms 0, 1, 3, 7, 15. This formula
is obviously true for \(n=0\), because \(2^0 - 1 = 0\).
Now, by the induction hypothesis, 
\[
T(n) = 2T(n-1)+1 = 2  (2^{n-1}-1) + 1 = 2^{n} - 1
\]
and this is exactly what we need to prove.
\end{Example}

The Fibonacci sequence provides a sterner test for our guessing abilities.
\begin{Example}
\label{ex:fibonacci:formula}
The first few terms of the sequence 
1, 1, 2, 3, 5, 8, 13, 21, 34, \ldots give
no hint regarding the desired explicit form. Thus let us
analyse the recurrence 
\(F(n)=F(n-1)+F(n-2)\) itself. \(F(n)\) is almost
doubled every time, so that \(F(n) < 2^n\). The simplest
guess \(F(n) = c2^n\) with \(c < 1\) 
fails because for any scaling factor 
\(c\) it leads to the impossible equality \(2^n = 2^{n-1}+2^{n-2}\),
or \(4 = 2 + 1\). The next guess is
that the base 2 of the
exponential function should be smaller, \(\phi < 2\),
that is, \(F(n) = c\phi^n\). The resulting
equation \(\phi^n = \phi^{n-1} + \phi^{n-2}\) 
reduces to the quadratic one, \(\phi^2 - \phi - 1 = 0\),
with the two roots:
\(\phi_{1} = 0.5 (1+\sqrt{5})\) and 
\(\phi_{2} = 0.5 (1-\sqrt{5})\). Because
each root solves the recurrence,
the same holds for any linear combination of them, so we know that
\(
F(n) = c_1 \phi_{1}^n + c_2 \phi_{2}^n
\)
satisfies the recurrence. We choose the constants 
 \(c_1\) and \(c_2\) to satisfy 
the base conditions \(F(1)=1\) and \(F(2)=1\):
\(F(1) = c_1 \phi_1 + c_2 \phi_2 = 1\)  and  
\(F(2) = c_1 \phi_{1}^2 + c_2 \phi_{2}^2 = 1\).
Thus \(c_1 = \frac{1}{\sqrt{5}}\) and
\(c_2 = -\frac{1}{\sqrt{5}}\) so that 
\[
F(n) = \frac{1}{\sqrt{5}}
\left (
\frac{1+\sqrt{5}}{2}
\right )^n - \frac{1}{\sqrt{5}}
\left (
\frac{1-\sqrt{5}}{2}
\right )^n \cong \frac{\phi^{n}}{\sqrt{5}}
\] 
where
$\phi = \frac{1+\sqrt{5}}{2} \cong 1.618$ is the 
well-known ``golden ratio''. The term
with $(1-\sqrt{5})/2 \cong -0.618$ tends to zero when
\(n \rightarrow \infty\), and so $F(n)$ is $\Theta(\phi^n)$.
\end{Example}


\subsection{``Telescoping'' of a recurrence}
This means a recursive substitution of the same implicit
relationship in order to derive the explicit relationship. Let us
apply it to the same recurrence \(T(n) = 2T(n-1)+1\) with
the base condition \(T(0) = 0\) as in Examples~\ref{ex:exponential}
and \ref{ex:exp:recurrence}:
\begin{description}
\item[Step 0] \boldfont{initial recurrence} \(T(n) = 2T(n-1)+1\) 
\item[Step 1] \boldfont{substitute} \(T(n-1)=2T(n-2) + 1\), that is,
replace \(T(n-1)\):
\begin{eqnarray*}
   T(n) =  2(2T(n-2) + 1) + 1 =  2^2 T(n-2) + 2 + 1
\end{eqnarray*}
\item[Step 2] \boldfont{substitute} \(T(n-2)=2T(n-3) + 1\):
\begin{eqnarray*}
   T(n) =  2^2 (2T(n-3) + 1) + 2 + 1 
        = 2^3 T(n-3) + 2^2 + 2 + 1
\end{eqnarray*}
\item[Step 3] \boldfont{substitute} \(T(n-3)=2T(n-4) + 1\):
\begin{eqnarray*}
   T(n) & = & 2^3 (2T(n-4) + 1) + 2^2 + 2 + 1 \\
        & = & 2^4 T(n-4) + 2^3 + 2^2 + 2 + 1
\end{eqnarray*}
\item[Step \ldots] \boldfont{\ldots}
\item[Step $n-2$]  \boldfont{\ldots}
\begin{eqnarray*}
   T(n) & = & 2^{n-1}T(1) + 2^{n-2} + \ldots + 2^2 + 2 + 1
\end{eqnarray*}
\item[Step $n-1$] \boldfont{substitute} \(T(1) = 2T(0) + 1\):
\begin{eqnarray*}
   T(n) & = & 2^{n-1} (2T(0) + 1) + 2^{n-2} + \ldots + 2 + 1 \\
        & = & 2^{n}T(0) + 2^{n-1} + 2^{n-2} + \ldots + 2 + 1
\end{eqnarray*}
\end{description} 
Because of the base condition \(T(0)=0\), the explicit formula is:
\[
T(n) = 2^{n-1} + 2^{n-2} + \ldots + 2 + 1 \equiv 2^n - 1
\]

As shown in Figure~\ref{f:telescope}, rather than
successively substitute the terms \(T(n-1)\), \(T(n-2)\),
\ldots, \(T(2)\), \(T(1)\), it is more convenient to write 
down a sequence of
the scaled relationships for \(T(n)\), \(2T(n-1)\), \(2^2 T(n-2)\),
\ldots, \(2^{n-1} T(1)\), respectively, then individually
sum left and right columns, and eliminate
similar terms in the both sums (the terms are scaled to 
facilitate their direct elimination). Such solution is 
called \defnfont{telescoping} because the recurrence unfolds
like a telescopic tube. 
 
\begin{figure}[htb!]
 \centerline{\illustr{tele-sup.eps}{110mm}}
    \caption{\label{f:telescope}Telescoping as a recursive substitution.}
\end{figure}

Although telescoping is not a powerful technique, 
it returns the desired explicit 
forms of most of the basic recurrences that we need in this book (see 
Examples~\ref{exm:recur:a}--\ref{exm:recur:d} below). But
it is helpless in the case of the Fibonacci 
recurrence because after proper scaling of terms
and reducing similar terms in the left and right sums,
telescoping returns just the same initial recurrence.

%\newpage

\begin{Example} \label{exm:recur:a}
$ T(n)=T(n-1)+n; T(0)=1$.

This relation arises when a recursive algorithm loops 
through the input to eliminate one item and 
is easily solved by telescoping:
\begin{eqnarray*}
    T(n) & = & T(n-1) + n\\
    T(n-1) & = & T(n-2) + (n-1)\\
           &\ldots & \\
    T(1)   & = & T(0) + 1
\end{eqnarray*}
By summing left and right columns and eliminating the 
similar terms, we obtain that
\(
T(n) = T(0)+1+2+\ldots+(n-2)+(n-1)+n = \frac{n(n+1)}{2}\) 
so that \(T(n)\) is \(\Theta(n^2 )\).
\end{Example}

\begin{Example} \label{exm:recur:b}
$T(n)=T\left (\lceil n/2 \rceil\right )+1; T(1)=0.$

This relation arises for a
recursive algorithm that almost halves the input 
at each step. Suppose first that $n=2^{m}$. Then,
the recurrence telescopes as follows:
\begin{eqnarray*}
    T(2^{m}) & = & T(2^{m-1}) + 1\\
    T(2^{m-1}) & = & T(2^{m-2}) + 1\\
           &\ldots & \\
    T(2^{1})   & = & T(2^{0}) + 1
\end{eqnarray*}
so that $T(2^{m}) = m$, or $T(n) = \lg n$ which is $\Theta(\log n)$.

For general $n$, the total number of the halving steps cannot be greater than 
\(m=\lceil \lg n \rceil\). Therefore, $T(n) \le \lceil \lg n\rceil$ for all $n$. 
This recurrence is usually called the \defnfont{repeated halving principle}.
\end{Example}

\begin{Example} \label{exm:recur:c}
Recurrence $T(n)=T(\lceil n/2\rceil )+n; T(0)=0$.

This relation arises for a
recursive algorithm that halves the input after
examining every item in the input for \(n \ge 1\).
Under the same simplifying assumption $n=2^{m}$,
the recurrence telescopes as follows:
\begin{eqnarray*}
    T(2^{m}) & = & T(2^{m-1}) + n\\
    T(2^{m-1}) & = & T(2^{m-2}) + n/2\\
    T(2^{m-2}) & = & T(2^{m-3}) + n/4\\
           &\ldots & \\
    T(2)   & = & T(1) +2\\
    T(1)   & = & T(0) +1
\end{eqnarray*}
so that $T(n)=n+\frac{n}{2}+\frac{n}{4}+\ldots+1$ 
which is $\Theta(n)$.

In the general case, the solution is also $\Theta(n)$ because
each recurrence after halving an odd-size input may
add to the above sum at most 1 and the number of these
extra units is at most \(\lceil \lg n \rceil\). 
\end{Example}

\begin{Example} \label{exm:recur:d}
Recurrence $T(n)= T(\lceil n/2 \rceil)+T(\lfloor n/2 \rfloor) + n; T(1)=0$. 

This relation arises for a
recursive algorithm that makes a linear pass through 
the input for \(n \ge 2\) and splits it into two halves.
Under the same simplifying assumption $n=2^{m}$,
the recurrence telescopes as follows:
\begin{eqnarray*}
    T(2^{m}) & = & 2 T(2^{m-1}) + 2^{m}\\
    T(2^{m-1}) & = & 2 T(2^{m-2}) + 2^{m-1}\\
           &\ldots & \\
    T(2)  & = & 2 T(1) + 2
\end{eqnarray*}
so that
\begin{eqnarray*}
  \frac { T(2^{m}) } { 2^{m} } & = & \frac { T(2^{m-1}) } {2^{m-1} } + 1\\
  \frac { T(2^{m-1}) } { 2^{m-1} }& = & \frac{ T(2^{m-2})} { 2^{m-2} } + 1\\
           &\ldots & \\
  \frac { T(2) }{ 2 }   & = & \frac{ T(1)}{ 1 } +1.
\end{eqnarray*}
Therefore, $\frac{T(n)}{n} = \frac{T(1)}{1} + m = \lg n$, so that
$T(n) = n \lg n$ which is $\Theta(n \log n)$.

For general $n$, $T(n)$ is also $\Theta(n \log n)$ (see Exercise~\ref{exr:rec-mergesort}).
\end{Example}


There exist very helpful parallels between the
differentiation / integration in calculus and 
recurrence analysis by telescoping.
\begin{itemize}
\item The difference equation $T(n)- 2 T(n-1)= c$ 
rewritten as $\frac{T(n)-T(n-1)}{1} = T(n-1) + c$
resembles the differential equation $\frac{dT(x)}{dx} = T(x)$. 
Telescoping of the difference equation results in
the formula $T(n) = c(2^{n}-1)$ whereas the
integration of the differential equation produces the analogous 
exponential one $T(x) = c e^{x}$.
\item The difference equation  $T(n)-T(n-1)= c  n$  has the
differential analogue $\frac{dT(x)}{dx} = cx$, and both
equations have similar solutions $T(n) = c\frac{n(n+1)}{2}$ and
$T(x) = \frac{c}{2}x^{2}$, respectively.
\item 
Let us change variables
by replacing $n$ and $x$ with $m = \lg n$ and
$y = \ln x$ so that $n=2^{m}$ and $x=e^{y}$, 
respectively. The difference equation $T(n)-T(\frac{n}{2})= c $ 
where $n = 2^{m}$ and $\frac{n}{2} = 2^{m-1}$ reduces to
$T(m)-T(m-1) = c $. The latter
has the differential analogue $\frac{dT(y)}{dy} = c $. These
two equations result in the similar explicit expressions 
$T(m) = cm$ and $T(y) = cy$, respectively, so that
$T(n) = c\lg n$ and $T(x) = c \ln x$. 
\end{itemize}
The parallels between  difference and differential equations
may help us in deriving the desired closed-form
solutions of complicated recurrences.


\begin{Exercise}\label{exr:rec-low-bound}
Show that the solution in Example~\ref{exm:recur:c} is also in $\Omega(n)$ 
for general $n$.

\end{Exercise}

\begin{Exercise} \label{exr:rec-mergesort} 
Show that the solution $T(n)$ to Example~\ref{exm:recur:d} is no more than 
$n\lg n + n - 1$ for every $n\geq 1$. Hint: try induction on $n$.
\end{Exercise}

\begin{Exercise}\label{exr:recur:cn}
The running time $T(n)$ of a certain algorithm to process $n$ data items
is given by the recurrence
\(T(n) = kT \left ( \frac{n}{k} \right ) + c n\);
\(T(1) = 0\)
where $k$ is a positive integer constant and $n/k$ means either 
$\lceil n/k \rceil$ or $\lfloor n/k \rfloor$.
Derive the explicit expression
for $T(n)$ in terms of $c$, $n$, and $k$ assuming
$n = k^{m}$ with integer $m= \log_{k}n$ and $k$ and 
find time 
complexity of this algorithm in the ``Big-Oh'' sense. 
\end{Exercise}

\begin{Exercise}\label{exr:recur:ckn}
The running time \(T(n)\) of a slightly different algorithm 
is given by the recurrence
\(T(n) = k  T \left ( \frac{n}{k} \right ) + c k n\);
\(T(1) = 0\).
Derive the explicit expression
for $T(n)$ in terms of $c$, $n$, and $k$ under the same
assumption $n = k^{m}$ and find time 
complexity of this algorithm in the ``Big-Oh'' sense.
\end{Exercise}

\if 01
\begin{Exercise} [General divide-and-conquer recurrence]
\label{exr:recur:general:dc}

A divide-and-conquer algorithm dissects a large problem into
smaller subproblems and recursively combines their solutions in
order to solve the initial problem.
Let a
problem of size \(n\) be split into \(a\) subproblems,
each of size \(\frac{n}{b}\). Let \(cn^k\) 
be time of combining solutions of the subproblems.
For particular constants \(a\), \(b\), \(c\), and \(k\),
running time \(T(n)\) of the algorithm satisfies the
recurrence
\[
T(n) = aT\left(\frac{n}{b}\right ) + c n^k; \;\; T(1) = c
\]
Assuming, for simplicity,
\(n = b^m\), derive explicit expressions for \(T(n)\) 
in the three cases: \(b^k < a\), \(b^k = a\), and \(b^k > a\)
and prove the general divide-and-conquer 
\begin{theorem}
The recurrence $T(n) = aT(n/b)+cn^k$, where $a\ge 1$ and $b\ge 2$
are integer constants and
$c>0$ and $k>0$ are real constants, has the following solution:
\[
T(n) = \left \{
\begin{array}{lll}
O(n^{\log_b a}) & \mathrm{if} & b^k < a\\
O(n^k \log n)   & \mathrm{if} & b^k = a\\ 
O(n^k )         & \mathrm{if} & b^k > a\\
\end{array}
\right .
\]
\end{theorem}
The theorem applies to a large number of divide-and-conquer
algorithms.
Determine how a different base condition, \(T(1)=d \ne c\), may effect
this theorem.
\end{Exercise}
\fi
 
\section{Capabilities and limitations of algorithm analysis}
\label{sec:capab:limit}

We should neither overestimate nor underestimate the
capabilities of algorithm analysis. Many existing
algorithms have been analysed with  much more
complex techniques than used in this book and recommended 
for practical use on the basis of these studies.
Of course, not all algorithms are worthy of study and
we should not suppose that a rough complexity
analysis will result immediately in efficient algorithms. But
computational complexity permits us to better
evaluate basic ideas in developing new algorithms.

To check whether our analysis is correct, we may code
a program and see whether its observed running time fits
predictions. But it is very
difficult to differentiate between, say, 
$\Theta(n)$ and $\Theta(n \log n)$
algorithms using purely empirical evidence. 
Also, ``Big-Oh'' 
analysis is not appropriate for small amounts of input and
hides the constants which may be crucial for a particular
task. 

\begin{Example}
An algorithm \textbf{A} with running time $T_{\mathbf{A}}=2n \lg n$
becomes less efficient than another algorithm \textbf{B} having 
running time $T_{\mathbf{B}}=1000n$
only when \(2n \lg n > 1000 n\), or
$ \lg n > 500$, or \(n > 2^{500} \cong 10^{150}\),
and such amounts of input data simply cannot be met in practice.
Thus, although the algorithm \textbf{B} is better in the
``Big Oh'' sense, in practice we should use algorithm \textbf{A}.
\end{Example}


Large constants have to be taken into account when
an algorithm is very complex, or when we must discriminate
between cheap or expensive access to input data items,
or when there may be lack of sufficient memory for storing
large data sets, etc. But even when constants and lower-order
terms are considered, the performance predicted by our analysis 
may differ from the empirical results. Recall that for \boldfont{very} large 
inputs, even the asymptotic analysis may break down, because some operations 
(like addition of large numbers) can no longer be considered as elementary.

In order to analyse algorithm performance we have used a simplified mathematical 
model involving elementary operations. In the past, this allowed for fairly 
accurate analysis of the actual running time of program implementing a given 
algorithm. Unfortunately, the situation has become more complicated in recent 
years. Sophisticated behaviour of computer hardware such as pipelining and 
caching means that the time for elementary operations can vary wildly, 
making these models less useful for detailed prediction. Nevertheless, the basic
distinction between linear, quadratic, cubic and exponential time is still as 
relevant as ever. In other words, the crude differences captured by the Big-Oh 
notation give us a very good way of comparing algorithms; 
comparing two linear time algorithms, for example, will require more 
experimentation.

We can use worst-case and average-case analysis to obtain some meaningful estimates
of possible algorithm performance. But we must remember that both
recurrences and asymptotic ``Big-Oh'', ``Big-Omega'', and
``Big-Theta'' notation are just mathematical tools used to model certain
aspects of algorithms. Like all models, they are not universally valid
and so the mathematical model and the real algorithm may behave quite
differently.

\subsection*{Exercises}

\begin{Exercise}
\label{exr:alg-compar:1}
Algorithms \textbf{A} and \textbf{B} 
use \(T_\mathbf{A}(n) = 5n\log_{10}n\)
and \(T_\mathbf{B}(n) = 40n\) elementary operations, respectively,
for a problem of size \(n\). 
Which algorithm has better performance in the ``Big Oh'' sense? 
Work out exact conditions when each algorithm outperforms the other.
\end{Exercise}

\begin{Exercise}
\label{exr:alg-compar:2} We have to choose one of two algorithms, 
\textbf{A} and \textbf{B}, to process a database
containing $10^{9}$ records. The average running time 
of the algorithms is $T_{\mathbf{A}}(n) = 0.001n$ and
$T_{\mathbf{B}}(n) = 500\sqrt{n}$, 
respectively. Which algorithm  should be used, assuming the application is such 
that we can tolerate the risk of an occasional long running time?
\end{Exercise}


\section{Notes}

The word \emph{algorithm} relates to the surname of the great 
mathematician Mu\-ham\-mad ibn Musa al-Khwarizmi, whose life spanned approximately 
the period 780--850. His works, translated
from Arabic into Latin, for the first time exposed Europeans to new 
mathematical ideas such as the Hindu positional decimal notation and 
step-by-step rules for addition, subtraction, multiplication, and division of
 decimal numbers. The translation converted his surname into ``Algorismus'', 
 and the computational
rules took on this name. Of course, mathematical algorithms existed well  
before the term itself. For instance, Euclid's algorithm for
computing the greatest common divisor of two positive integers was devised over
1000 years before. 

The Big-Oh notation was used as long ago as 1894 by Paul Bachmann and then
 Edmund Landau for use in number theory. However the 
other asymptotic notations Big-Omega and Big-Theta were introduced in 1976 by 
Donald Knuth (at time of writing, perhaps the world's greatest living computer 
scientist).

Algorithms  running in $\Theta(n\log n)$ time are sometimes called 
\defnfont{linearithmic}, to match ``logarithmic", ``linear", ``quadratic", etc. 

The quadratic equation for \(\phi\) in Example~\ref{ex:fibonacci:formula}
is called the \defnfont{characteristic equation} of the recurrence.
A similar technique can be used for solving any constant coefficient linear 
recurrence of the form \(F(n)=\sum_{k=1}^{K}a_{k}F(n-k)\) where $K$ is a 
fixed positive integer and the $a_k$ are constants. 

