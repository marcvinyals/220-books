\part{What is Algorithm Analysis?} \label{ch:alganal}

\chapter{Lecture: What is an algorithm and why analyse it?}

\chapter{Lecture: How to measure running time?}

\chapter{Lecture: Asymptotic notation}

% ------vvv inserted from book vvv---------
Two simple concepts separate properties of an algorithm itself from
properties of a particular computer, operating system, programming
language, and compiler used for its implementation. The concepts,
briefly outlined earlier, are as follows: 
\begin{itemize}
 \item The \defnfont{input data size}, or
the number $n$ of individual data items in a single data instance to be
processed when solving a given problem. Obviously, how to measure the data
size depends on the problem: \(n\) means the number of items to sort (in sorting
applications),
number of nodes (vertices) or arcs (edges) in graph algorithms, number
of picture elements (pixels) in image processing, length of a character
string in text processing, and so on.  
\item The number of elementary operations taken by a particular algorithm, or its
running time. We assume it is a function $f(n)$ of the input data size
$n$. The function depends on the elementary operations chosen to build the
algorithm. 
\end{itemize} 

The running time of a program which implements
the algorithm is $c  f(n)$ where $c$ is a constant factor depending
on a computer, language, operating system, and compiler. Even if we don't know 
the value of the factor \(c\), we are able to answer the important question:
\emph{if the input size increases from $n=n_{1}$ to $n=n_{2}$, how does
the relative running time of the program change, all other things being equal?}
The answer is obvious: the 
running time increases by a factor of 
\( 
\frac{T(n_{2})}{T(n_{1})} = 
\frac{c  f(n_{2})}{c  f(n_{1})} =  
\frac{f(n_{2})}{f(n_{1})}
\). 

As we have already seen, the approximate running time for large input
sizes gives enough information to distinguish between a good and a bad
algorithm. Also, the constant $c$ above can rarely be determined. We
need some mathematical notation to avoid having to say ``\emph{of the
order of} $\ldots$''  or ``\emph{roughly proportional to} $\ldots$'',
and to make this intuition precise.

The standard mathematical tools ``\emph{Big Oh}'' ($O$), ``\emph{Big
Theta}'' ($\Theta$), and ``\emph{Big Omega}'' ($\Omega$) do 
precisely this.

\begin{note}
Actually, the above letter $O$ is a capital ``omicron''  (all
letters in this notation are Greek letters). However, since the Greek
omicron and the English ``O'' are indistinguishable in most fonts, we
read $O()$ as ``Big Oh'' rather than ``Big Omicron''. 
\end{note}

The algorithms are analysed under the following assumption: \emph{if
the running time of an algorithm as a function of $n$ differs only by a
constant factor from the  running time for another algorithm, then the
two algorithms have essentially the same time complexity.} Functions that
measure running time, $T(n)$, have nonnegative values because time is
nonnegative, $T(n) \ge 0$. The integer argument $n$ (data size) is also
nonnegative.

\begin{Definition} [Big Oh]
\label{def:oh}
Let $f(n)$ and $g(n)$ be nonnegative-valued functions defined on
nonnegative integers $n$. Then $g(n)$ is $O(f(n))$ (read ``\(g(n)\) is
Big Oh of \(f(n)\)'') iff there exists 
a positive real constant $c$ and
a positive integer $n_{0}$ such that $g(n) \le c f(n)$ for all $n>n_{0}$.
\end{Definition}
\begin{note} 
We use the notation ``\defnfont{iff}'' as an abbreviation of ``if and only if''.
\end{note}
In other words, if $g(n)$ is $O(f(n))$ then an algorithm with running time 
$g(n)$ runs for large $n$ at least as fast, to within a
constant factor, as an algorithm with running time  $f(n)$. Usually
the term ``\defnfont{asymptotically}'' is used in this context to describe 
behaviour of functions for sufficiently large values of $n$. This term 
means that \(g(n)\) for large \(n\) may approach closer and closer to 
\(c\cdot f(n)\). Thus, \(O(f(n))\) specifies
an \defnfont{asymptotic upper bound}.
\begin{note} 
Sometimes the ``Big Oh'' property is denoted $g(n)=O(f(n))$, but
we should not assume that the function $g(n)$ is equal to something 
called ``Big Oh'' of $f(n)$. This notation really means $g(n) \in O(f(n))$, 
that is, \(g(n)\) is a member of the set $O(f(n))$ 
of functions which are increasing, in essence, with the same 
or lesser rate as  \(n\) tends to infinity ($n \rightarrow \infty$). 
In terms of graphs of these functions, $g(n)$ is $O(f(n))$ iff
there exists a constant $c$ such that the graph of $g(n)$ is always
below or at the graph of $cf(n)$ after a certain point, $n_{0}$. 
\end{note}

\begin{Example}
Function \(g(n)=100\log_{10}n\) in Figure~\ref{f:aa-graphs} 
is \(O(n)\) because the graph \(g(n)\) is always below
the graph of \(f(n)=n\) if \(n > 238\) or of \(f(n)=0.3n\)
if \(n > 1000\), etc. 
\end{Example}

\begin{figure}[htb!]
\centerline{
\illustr{aa-graphs.eps}{80mm}}
\caption{``Big Oh'' property: $g(n)$ is $O(n)$.}
\label{f:aa-graphs}
\end{figure}
\begin{Definition} [Big Omega]
\label{def:omega}
The function $g(n)$ is $\Omega(f(n))$ iff 
there exists a positive real constant $c$ and a 
positive integer $n_{0}$  
such that $g(n) \ge c f(n)$ for all $n>n_{0}$.
\end{Definition} 

``Big Omega'' is complementary to ``Big Oh'' and generalises the
concept of ``lower bound'' (\(\ge\)) in the same way as ``Big Oh''
generalises the concept of ``upper bound'' (\(\le\)): if
\(g(n)\) is \(O(f(n))\) then \(f(n)\) is \(\Omega(g(n))\), and vice versa. 

\begin{Definition} [Big Theta]
\label{def:theta}
The function $g(n)$ is $\Theta(f(n))$ iff 
there exist two positive real constants $c_1$ and $c_2$ and a 
positive integer $n_{0}$  
such that $c_1f(n) \le g(n) \le c_2 f(n)$ for all $n>n_{0}$.
\end{Definition}

Whenever two functions, \(f(n)\) and \(g(n)\), are actually of the same
order, \(g(n)\) is \(\Theta(f(n))\), they are each ``Big Oh'' of the
other: $f(n)$ is $O(g(n))$ and $g(n)$ is  $O(f(n))$. In other words,
\(f(n)\) is both an asymptotic upper and lower bound for \(g(n)\). The
``Big Theta'' property means \(f(n)\) and \(g(n)\) have asymptotically
tight bounds and are in some sense equivalent for our purposes.

In line with the above definitions,
\(g(n)\) is \(O(f(n))\) iff \(g(n)\) grows \boldfont{at 
most} as fast as \(f(n)\) to within a constant factor, \(g(n)\) is
\(\Omega(f(n))\) iff \(g(n)\) grows \boldfont{at 
least} as fast as \(f(n)\) to within a constant factor, and
\(g(n)\) is \(\Theta(f(n))\) iff \(g(n)\) and \(f(n)\) grow
\boldfont{at the same rate} to within a constant factor.


``Big Oh'', ``Big Theta'', and ``Big Omega'' notation formally capture 
two crucial ideas in comparing algorithms:
the exact function, $g$, is not very
important because it can be multiplied by any
arbitrary positive constant, $c$, and
the relative behaviour of two functions
is compared only asymptotically, for large $n$, but not near the
origin where it may make no sense.
Of course, if the constants involved are very large, 
the asymptotic behaviour loses practical interest. In most
cases, however, the constants remain fairly small. 

In analysing running time,
``Big Oh'' $g(n) \in O(f(n))$, ``Big Omega'' $g(n) \in \Omega(f(n))$,
and ``Big Theta'' $g(n) \in \Theta(f(n))$  
definitions are mostly used with $g(n)$ equal to 
``exact'' running time on inputs of size $n$ and 
$f(n)$ equal to a rough approximation to
running time (like \(\log n\), \(n\), \(n^2\), and so on).
 
To prove that some
function \(g(n)\) is \(O(f(n))\), \(\Omega(f(n))\), or 
\(\Theta(f(n))\) using the definitions we need to find the constants \(c\), \(n_0\) or \(c_1\), \(c_2\), \(n_0\) specified in 
Definitions~\ref{def:oh}, \ref{def:omega}, \ref{def:theta}. 
Sometimes the proof is given only by a chain of inequalities,
starting with \(f(n)\). In other cases it
may involve more intricate techniques, such as mathematical
induction. Usually the manipulations
are quite simple. To prove that \(g(n)\) is \boldfont{not} 
\(O(f(n))\), \(\Omega(f(n))\), or \(\Theta(f(n))\) we have 
to show the desired constants do not exist, that is,
their assumed existence leads to a contradiction.

\begin{Example}
To prove that 
linear function $g(n) = an + b$; $a > 0$, is $O(n)$, we
form the following chain of inequalities:  
\(g(n) \le an + |b| \le (a+|b|)n\) for all \(n \ge 1\).
Thus, Definition~\ref{def:oh}
with \(c=a+|b|\) and \(n_0 = 1\) shows 
that \(an + b\) is \(O(n)\). 
\end{Example} 
 
``Big Oh'' hides constant factors so that both $10^{-10}n$ and
$10^{10}n$ are $O(n)$. It is pointless to write something like  $O(2 
n)$ or $O(a  n + b)$ because this still means $O(n)$. Also, only 
the dominant terms as  $n \rightarrow
\infty$ need be shown as the argument of ``Big Oh'', ``Big Omega'', or
``Big Theta''.

\begin{Example} 
    The polynomial $P_{5}(n) = a_{5}n^{5} + a_{4}n^{4}+a_{3}n^{3} 
+a_{2}n^{2}+a_{1}n +a_{0}$; $a_{5}>0$, is $O(n^{5})$ 
because  
\(P_{5}(n) \leq (a_{5}+|a_{4}|+|a_{3}|+|a_{2}|+|a_{1}|+|a_{0}|)n^{5}\)
for  all \(n \ge 1\). 
\end{Example}


\begin{Example} 
\label{ex:expons}
The exponential function $g(n) = 2^{n+k}$, where $k$ is a constant,
is $O(2^{n})$ because 
\(2^{n+k} = 2^{k}  2^{n}\) for all \(n\).
Generally, $m^{n+k}$ is $O(l^{n})$; $l \ge m > 1$, because 
$m^{n+k} \le l^{n+k} = l^{k}  l^{n}$ for any constant $k$. 
\end{Example}

\begin{Example} 
\label{ex:logs}
For each $m>1$, the logarithmic function $g(n) = \log_{m}(n)$ has the same rate of 
increase as $\lg(n)$ because 
\(\log_{m}(n) = \log_{m}(2)  \lg(n)\) for  all \(n > 0\).
Therefore we may omit the logarithm base when using 
the ``Big-Oh'' and ``Big Theta'' notation: \(\log_{m}n\) is \(\Theta(\log n)\).
\end{Example}


\subsection{Rules for asymptotic notation}
\label{o-features}
 
Using the definition to prove asymptotic relationships between functions is hard
work. As in calculus, where we soon learn to use various rules (product rule, 
chain rule, \dots) rather than the definition of derivative, we can use some 
simple rules to deduce new relationships from old ones.

We present rules for ``Big Oh"---similar relationships hold for ``Big Omega'' 
and ``Big Theta''.

We will consider the features both informally and formally using the
following notation. Let $x$ and $y$ be functions of a nonnegative
integer $n$. Then $z=x+y$ and $z=xy$ denote the sum of the functions,
$z(n) = x(n)+y(n)$, and the product function: $z(n) = x(n)y(n)$,
respectively, for every value of $n$. The product function $(xy)(n)$
returns the product of the values of the functions at $n$ and has
nothing in common with the composition $x(y(n))$ of the two functions.

Basic arithmetic relationships for ``Big Oh''
follow from and can be easily proven with its definition. 

\begin{Lemma}[Scaling] \label{l:bigoh:1}
For all constants $c > 0$, $c  f$ is $O(f)$. In particular, $f$ is $O(f)$. 
\end{Lemma}

\begin{proof}
The relationship \(cf(n) \leq c f(n)\) obviously holds for all \(n\geq 0\).
\end{proof}

Constant factors are ignored, and only the powers and functions are
taken into account. It is this ignoring of constant factors that
motivates such a notation.

\begin{Lemma}[Transitivity] \label{l:bigoh:2}
If $h$ is $O(g)$ and $g$ is $O(f)$, then $h$ is $O(f)$.
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

Informally, if $h$ grows at most as quickly as $g$, which grows 
at most as quickly as $f$, then $h$ grows at most as quickly as $f$. 

\begin{Lemma}[Rule of sums] \label{l:bigoh:3}
%{\textsc{Rule of sums.}} 
If $g_{1}$ is $O(f_{1})$ and 
$g_{2}$ is $O(f_{2})$, then 
$g_{1}+g_{2}$ is $O(\max \{ f_{1},f_{2} \} )$. 
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

If $g$ is $O(f)$ and $h$ is $O(f)$, then is $g+h$ is $O(f)$. In particular, 
if $g$ is $O(f)$, then $g+f$ is $O(f)$. Informally, the growth rate of a sum 
is the growth rate of its fastest-growing term.
 
%\newpage 

\begin{Lemma}[Rule of products]\label{l:bigoh:4}
%{\textsc{Rule of products.}}
 If $g_{1}$ is $O(f_{1})$ and 
$g_{2}$ is $O(f_{2})$, then 
$g_{1}  g_{2}$ is $O(f_{1}  f_{2})$. 
\end{Lemma}
\begin{proof}
See Exercise~\ref{exr:bigoh:features}.
\end{proof}

In particular, if $g$ is $O(f)$, then $g  h$ is $O(f  h)$. Informally,
the product of upper bounds of functions gives an upper bound for the
product of the functions.

Using calculus we can obtain a nice time-saving rule.

\begin{Lemma}[Limit Rule] \label{l:bigoh:lim}
Suppose $\lim_{n\to \infty} f(n)/g(n)$ exists (may be $\infty$), and denote 
the limit by $L$. Then
\begin{itemize}
\item if $L=0$, then $f$ is $O(g)$ and $f$ is not $\Omega(g)$;
\item if $0 < L < \infty$ then $f$ is $\Theta(g)$;
\item if $L = \infty$ then $f$ is $\Omega(g)$ and $f$ is not $O(g)$.
\end{itemize}
\end{Lemma}

\begin{proof}
If $L = 0$ then from the definition of limit, in particular there is some 
$n_0$ such that $f(n)/g(n) \leq 1$ for all $n \geq n_0$. Thus $f(n) \leq g(n)$ 
for all such $n$, and $f(n)$ is $O(g(n))$ by definition. On the other 
hand, for each $c > 0$, it is not the case that $f(n) \geq c g(n)$ for all
$n$ past some threshold value $n_1$, so that $f(n)$ is not $\Omega(g(n))$.
The other two parts are proved in the analogous way.
\end{proof}

To compute the limit if it exists, the standard \emph{L'H\^{o}pital's rule}
of calculus is useful (see Section~\ref{sec:app:lhopital}).

More specific relations follow directly from the basic ones.

\begin{Example} 
\label{ex:powers}
Higher powers of $n$ grow more quickly than lower powers:
$n^{k}$ is $O(n^{l})$ if $0 \le k \le l$. This follows directly from the limit rule
since $n^k/n^l = n^{k-l}$ has limit $1$ if $k=l$ and $0$ if $k<l$.
\end{Example}

\begin{Example}  
The growth rate of a polynomial is given by the growth rate of its leading 
term (ignoring the leading coefficient by the scaling feature): 
if $P_{k}(n)$ is a polynomial of exact degree $k$ then $P_{k}(n)$ is $\Theta(n^{k})$. 
This follows easily from the limit rule as in the preceding example.
\end{Example}


\begin{Example}  
Exponential functions grow more quickly than powers: 
$n^{k}$ is $O(b^{n})$, for all $b>1$, $n>1$, and $k \ge 0$. 
The restrictions on $b$, $n$, and $k$ merely ensure that both 
functions are increasing. This result can be proved by induction or by using the 
limit-L'H\^{o}pital approach above.
\end{Example}

\begin{Example}  
Logarithmic functions grow more slowly than powers: 
$\log_{b} n$ is $O(n^{k})$ for all $b>1$, $k > 0$. 
This is the inverse of the preceding feature. Thus, 
as a result, $\log n$ is $O(n)$ and $n \log n$ is $O(n^{2})$.  
\end{Example} 

\subsection*{Exercises}

\begin{Exercise}\label{exr:aa:bigO:a}
Prove that $10n^3 - 5n + 15$ is not $O(n^{2})$.
\end{Exercise}
\begin{Exercise}\label{exr:aa:bigO:b}
Prove that $10n^3 - 5n + 15$ is $\Theta(n^{3})$.
\end{Exercise}
\begin{Exercise}\label{exr:aa:bigO:c}
Prove that $10n^3 - 5n + 15$ is not $\Omega(n^{4})$.
\end{Exercise}

\begin{Exercise}\label{exr:bigtheta}
Prove that $f(n)$ is $\Theta(g(n))$ if and only if both $f(n)$ is $O(g(n)$ and 
$f(n)$ is $\Omega(g(n))$.
\end{Exercise}

\begin{Exercise}
\label{exr:bigoh-order}
Using the definition, show that each function \(f(n)\) in 
Table~\ref{t:data-size} stands in ``Big-Oh'' relation to the preceding 
one, that is, \(n\) is \(O(n\log n)\), \(n\log n\) is \(O(n^{1.5})\),
and so forth. 

\end{Exercise}

\begin{Exercise}\label{exr:bigoh:features}
Prove Lemmas~\ref{l:bigoh:2}--\ref{l:bigoh:4}.
\end{Exercise}

\begin{Exercise}\label{exr:bigomega:sums}
Decide on how to reformulate the Rule of Sums (Lemma~\ref{l:bigoh:3})
for ``Big Omega'' and ``Big Theta'' notation.
\end{Exercise}

\begin{Exercise}\label{exr:bigomega:lem}
Reformulate and prove Lemmas~\ref{l:bigoh:1}--\ref{l:bigoh:4}
for ``Big Omega'' notation. 
\end{Exercise}

\section{Time complexity of algorithms} 
\label{time-compl}

\begin{Definition} [Informal]
A function $f(n)$ such that the running time $T(n)$ of a given 
algorithm is $\Theta(f(n))$ measures the \defnfont{time complexity} 
of the algorithm.
\end{Definition}

An algorithm is called \defnfont{polynomial time} if its running time $T(n)$
is $O(n^{k})$ where $k$ is some fixed positive integer. A computational
problem is considered \defnfont{intractable} iff no deterministic
algorithm with polynomial time complexity exists for it. But many problems
are classed as intractable only because a polynomial solution is unknown,
and it is a very challenging task to find such a solution for one of them.
 
\begin{table}[htb] 
\caption[Relative growth of running time $T(n)$ when 
  the input size increases.]%
{\label{t:growth} Relative growth of running time $T(n)$ when 
  the input size increases from $n=8$ to $n=1024$ provided that $T(8)=1$.} 
  \centerline{
   \begin{tabular}{|c|c|cccc|c|} \hline 
   \multicolumn{2}{|c|}{\textbf{Time complexity}} &  
   \multicolumn{4}{|c|}{\textbf{Input size $n$}} & \textbf{Time} $T(n)$
\\ \cline{1-6} 
   \emph{Function} & \emph{Notation} & \emph{8} & \emph{32} & \emph{128} & \emph{1024} &
\\ \hline 
   Constant     & $1$      & 1  & 1  &   1 &   1 & 1 \\ 
\hline 
   Logarithmic  & $\lg n$ & 1  & 1.67  & 2.67  & 3.33 & \( \lg n / 3 \)\\ 
\hline 
   Log-squared  & $\lg^{2} n$ & 1 & 2.78  & 5.44 &  11.1 & \( \lg^{2} n / 9 \) 
\\ \hline 
   Linear       & $n$ & 1 & 4 & 16 & 128 & \( n / 8 \) \\ 
\hline 
   ``$n\log n$''& $n \lg n$ & 1 & 6.67 & 37.3 & 427 & \(n \lg n / 24\)
\\ \hline 
   Quadratic    & $n^{2}$ & 1 & 16 & 256 & 16384 & \(n^{2} / 64\)
\\ \hline 
   Cubic        & $n^{3}$ & 1 & 64 & 4096 & 2097152 & \(n^{3} / 512\)
\\ \hline 
   Exponential  & $2^{n}$ & 1 & $2^{24}$ & $2^{120}$ & $2^{1016}$ & \(2^{n-8}\)
\\ \hline 
   \end{tabular}} 
 \end{table} 

Table~\ref{t:growth} shows how the running time \(T(n)\) of algorithms
having different time complexity, $f(n)$, grows relatively with the
increasing input size $n$.  Time complexity functions are listed in
order such that $g$ is $O(f)$ if $g$ is above $f$: for example, the
linear function \(n\) is \(O(n\log n) \) and \(O(n^2 )\), etc. The 
asymptotic growth rate does not depend on the base of the logarithm, but the 
exact numbers in the table do --- we use $\log_{2} = \lg$ for simplicity.

\begin{table}[htbp] 
\caption[The largest data sizes $n$ that can be
processed by an algorithm.]%
{\label{t:data-size} The largest data sizes $n$ that can be
processed by an algorithm with time complexity $f(n)$
provided that $T(10)= 1$ minute.} 
\begin{center}
   \begin{tabular}{|c|r|r|r|r|r|r|} \hline
      & \multicolumn{6}{|c|}{\textbf{Length of time to run an algorithm}}\\
                \cline{2-7}
$f(n)$  & 1 minute & 1 hour & 1 day & 1 week & 1 year & 1 decade  \\
\hline
  $n$         & 10  & 600 & 14 400 & 100 800 & $5.26\times 10^{6}$  &
$5.26\times  10^{7}$  \\ \hline 
  $n\lg n$    & 10  & 250 & 3 997 & 23 100  & 883 895   & $7.64\times 10^6$
  \\ \hline 
  $n^{1.5}$    & 10  & 153 &  1 275 &   4 666 &    65 128 &
   302,409   \\ \hline 
  $n^{2}$      & 10  &  77 &    379 &   1 003 &     7 249 &
    22,932   \\ \hline 
  $n^{3}$      & 10  &  39 &    112 &     216 &       807 &
     1,738  \\ \hline 
  $2^{n}$      & 10  &  15 &     20 &      23 &        29 &
        32   \\ \hline
   \end{tabular}
   \end{center} 
 \end{table} 
 
Table~\ref{t:data-size} is even more expressive in showing
how the time complexity of an algorithm affects the size of problems the
algorithm can solve (we again use $\log_{2} = \lg$). A linear algorithm solving 
a problem of size $n=10$ in exactly one minute will process about $5.26$ million  
data items per year and 10 times more if we can wait a decade. 
But an exponential algorithm  with \(T(10)=1\) minute will deal
only with $29$ data items after a year of running and add only $3$
more items after a decade.  Suppose we have computers $10,000$ times
faster (this is approximately the ratio of a week to a minute). Then 
we can solve a problem $10,000$ times, $100$ times, or $21.5$ times 
larger than before if our algorithm is linear, quadratic, or cubic, 
respectively. But for exponential algorithms, our progress is much worse: 
we can add only $13$ more input values if $T(n)$ is $\Theta(2^n)$.

Therefore, if our algorithm has a constant, logarithmic, log-square, linear, 
or even ``$n \log n$'' time complexity we may be happy and start writing a 
program with no doubt that it will meet at least some practical demands. 
Of course, before taking the plunge, it is better to check whether the hidden 
constant $c$, giving the computation volume per data item, is sufficiently small
in our case. Unfortunately, order relations can be drastically
misleading: for instance, two linear functions $10^{-4}n$ and $10^{10}n$
are of the same order $O(n)$, but we should not claim an
algorithm with the latter time complexity as a big success.

Therefore, we should follow a simple rule: \emph{roughly estimate 
the computation volume per data item for the algorithms after comparing
their time complexities in a ``Big-Oh'' sense!} We may estimate the
computation volume simply by counting the number of elementary
operations per data item.

In any case we should be \boldfont{very} careful even with simple
quadratic or cubic algorithms, and especially with exponential
algorithms. If the running time is speeded up in Table~\ref{t:data-size} so that
it takes one \emph{second} per ten data items
in all the cases, then we will still wait about 12 \emph{days}
(\(2^{20}\equiv 1,048,576\) seconds) for processing only 30 items by the 
exponential algorithm. Estimate yourself whether it is
practical to wait until 40 items are processed.
 
In practice, quadratic and cubic algorithms cannot be used if the input
size exceeds tens of thousands or thousands of items, respectively,
and exponential algorithms should be avoided whenever possible unless we
always have to process data of very small size. Because even the most
ingenious programming cannot make an inefficient algorithm fast (we
would merely change the value of the hidden constant $c$ slightly, but
not the asymptotic order of the running time), it is better to spend
more time to search for efficient algorithms, even at the expense of a
less elegant software implementation, than to spend time writing a very
elegant implementation of an inefficient algorithm. 

\subsection{Worst-case and average-case performance}
\label{ss:worst-vs-avg}

We have introduced asymptotic notation in order to measure the running time of 
an algorithm. This is expressed in terms of elementary operations. ``Big Oh", 
``Big Omega" and ``Big Theta" notations allow us to state upper, lower
 and tight asymptotic bounds on running time that are independent of inputs and
implementation details. Thus we can classify algorithms by 
performance, and search for the ``best'' algorithms for solving a particular problem.  

However, we have so far neglected one important point. In general,
\emph{the running time varies not only according to the size of the
input, but the input itself}. The examples in Section~\ref{time-compl} 
were unusual
in that this was not the case. But later we shall see many examples
where it does occur. For example, some sorting algorithms take almost
no time if the input is already sorted in the desired order, but much
longer if it is not.

If we wish to compare two different algorithms for the same problem, it
will be very complicated to consider their performance on all possible
inputs. We need a simple measure of running time.

The two most common measures of an algorithm are the \defnfont{worst-case 
running time}, and the \defnfont{average-case running time}. 

The worst-case running time has several advantages.  If we can show,
for example, that our algorithm runs in time $O(n\log n)$ no matter
what input of size $n$ we consider, we can be confident that even if we
have an ``unlucky" input given to our program, it will not fail to run
fairly quickly. For so-called ``mission-critical" applications this is
an essential requirement. In addition, an upper bound on the worst-case
running time is usually fairly easy to find.

The main drawback of the worst-case running time as a measure is that it may be 
too pessimistic. The real running time might be much lower than
an ``upper bound'', the input data causing  the worst case may be
unlikely to be met in practice, and the constants $c$ and $n_{0}$ of the 
asymptotic notation  are unknown and may not be small.
There are many algorithms for which it is difficult to specify the worst-case 
input. But even if it is known, the inputs actually encountered in practice may 
lead to much lower running times. We
shall see later that the most widely used fast sorting
algorithm, quicksort, has worst-case quadratic
running time, $\Theta(n^{2})$, but its running time
for ``random" inputs encountered in practice is $\Theta(n \log n)$.

By contrast, the average-case running time is not as easy to define. The use of 
the word ``average" shows us that probability is involved. We need to specify a 
probability distribution on the inputs. Sometimes this is not too difficult. 
Often we can assume that every input of size $n$ is equally likely, and this 
makes the mathematical analysis easier. But sometimes an assumption of this sort
 may not reflect the inputs encountered in practice. Even if it does, the
average-case analysis may be a rather difficult mathematical challenge
requiring intricate and detailed arguments. And of course the worst-case 
complexity may be very bad even if the average case complexity is good, so there
 may be considerable risk involved in using the algorithm.

Whichever measure we adopt for a given algorithm, our goal is  to show
that its running time is $\Theta(f)$ for some function $f$ \boldfont{and}
there is no algorithm with running time $\Theta(g)$ for any function $g$
that grows more slowly than $f$ when $n \rightarrow \infty$. In this case
our algorithm is \defnfont{asymptotically optimal} for the given problem.

Proving that no other algorithm can be asymptotically better than
ours is usually a difficult matter: we must carefully construct a
formal mathematical model of a computer and derive a lower bound on the
complexity of every algorithm to solve the given problem. In this book
we will not pursue this topic much. If our analysis does show that an
upper bound for our algorithm matches the lower one for the problem,
then we need not try to invent a faster one.
% ------^^^ inserted from book ^^^---------