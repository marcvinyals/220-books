\part{What is Algorithm Analysis?} \label{ch:alganal}


\chapter{What is an algorithm and why analyse it?} %--------------------------

An \emph{algorithm} is a sequence of clearly stated rules that
specify a step-by-step method for solving a given problem.
The rules should be unambiguous and
sufficiently detailed that they can be carried out without creativity.

Examples of algorithms are a (sufficiently detailed) cake recipe,
primary school method for multiplication of decimal integers, quicksort.

Algorithms predate electronic computers by thousands of years, for example
Euclid's greatest common divisor algorithm.

A \emph{program} is a sequence of computer instructions implementing the algorithm.

Experience shows that enormously more performance gains can be achieved 
by optimizing the algorithm than by optimizing other factors such as
processors, languages, compilers, or human programmers.
The analysis process often results in us discovering simpler algorithms.
Many algorithms have parameters that must be set before implementation. 
Analysis allows us to set the optimal values.
Algorithms that have not been analysed for correctness often lead to 
major bugs in programs.


\begin{Example}
The Fibonacci sequence is recursively defined by 
$$
F(n) = 
  \left\{\begin{array}{ll}
	n & \quad \text{if } n = 0 \text{ or } n = 1; \\ 
    F(n - 1) + F(n - 2) & \quad \text{if } n \geq 2 \text{.}
  \end{array}
  \right.
$$
This immediately suggests a recursive algorithm.
\end{Example}

\begin{algorithm}[H]
  \caption{Slow method for computing Fibonacci numbers} 
  \label{alg:slowfib}
\begin{algorithmic}[1]
\Function{slowfib}{integer $n$}
\If{$n < 0$} 
\Return{$0$}
\ElsIf{$n = 0$} 
\Return{$0$}
\ElsIf{$n = 1$} 
\Return{$1$}
\Else{ \Return{\Call{slowfib}{$n-1$} + \Call{slowfib}{$n-2$}}}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

The algorithm \texttt{slowfib} %(\cref{alg:slowfib}) 
is obviously correct, but does a lot of repeated computation. 
With a small (fixed) amount of extra space, we can do better, by 
working from the bottom up instead of from the top down.

\begin{algorithm}[H]
  \caption{Fast method for computing Fibonacci numbers
    \label{alg:fastfib}}
\begin{algorithmic}[1]
\Function{fastfib}{integer $n$}
\If{$n < 0$} 
\Return{$0$}
\ElsIf{$n = 0$} 
\Return{$0$}
\ElsIf{$n = 1$} 
\Return{$1$}
\Else
\Let{$a$}{$1$} \Comment{stores $F(i)$ at bottom of loop}
\Let{$b$}{$0$}  \Comment{stores $F(i-1)$ at bottom of loop}
\For{$i \gets 2$ \text{to} $n$}
\Let{$t$}{$a$}
\Let{$a$}{$a+b$}
\Let{$b$}{$t$}
\EndFor
\EndIf
\State \Return{$a$}
\EndFunction
\end{algorithmic}
\end{algorithm}
 
Proving correctness of \texttt{fastfib} is done by induction on $n$.  We omit the proof here.
%\item In fact even a bad implementation in a slow interpreted language on an 
%ancient machine of \texttt{fastfib} will beat the best implementation of 
%\texttt{slowfib}, once $n$ becomes big enough.
It is easy to see that the number of additions, function calls, etc needed by 
\texttt{fastfib} to compute $F(n)$ has the form $An+B$ for some constants $A, B$.

TODO? add extra questions from slides?



\chapter{How to measure running time} %---------------------------------------

There are three main characteristics of an algorithm designed to solve a given 
problem.
\begin{itemize}
\item Domain of definition: the set of legal inputs. 
\item Correctness: it gives correct output for each legal input. 
This depends on the problem we are trying to solve, and can be tricky to prove. 
\item Resource use: usually computing time and memory space. 
\begin{itemize} 
\item This depends on the input, and on the implementation 
(hardware, programmer skill, compiler, language, ...). 
\item It usually grows as the input size grows. 
\item There is a tradeoff between resources (for example, time vs space). 
\item Running time is usually more important than space use. 
\end{itemize}
\end{itemize}

In this course we mainly consider how to estimate resource use of an algorithm, 
ignoring implementation as much as possible.  

Given an algorithm $\algo$, the actual running time on a given
input $\inp$ depends on many implementation details. This is not desirable
because it does not allow direct comparison of algorithms. 
The running time usually grows with the size of the input.
Running time for very small inputs is not usually important; it is large
inputs that cause problems if the algorithm is inefficient. 
%\item We usually don't want to have to consider the distribution of
%running time over all possible inputs of a given size. There may be (infinitely)
% many inputs of a given size, and running time may vary widely on these. 

We use a clean mathematical model of the problem. Example: sorting 
distinct records is really the same problem as computing the inverse of a 
permutation. We define a notion of  \emph{input size} on the data. This is a positive integer. 
Example: number of records in a database to be sorted.

We use the concept of \emph{elementary operation} as our basic measuring 
unit of running time. This is any operation whose execution time does not depend on the size of the input.
The running time $T(\inp)$ of algorithm $\algo$ on input $\inp$ is the 
number of elementary operations used when $\inp$ is fed into $\algo$.
%\item We usually don't want to have to consider the distribution of
%running time over all possible inputs of a given size. There may be (infinitely)
% many inputs of a given size, and running time may vary widely on these. 
%\item We consider statistics of $T(\inp)$ such as \emph{worst-case} $W(n)$ or 
%\emph{average-case} $A(n)$ running time for instances $\inp$ of size $n$.
%\item We consider only \emph{asymptotic} (large-$n$) comparison of running 
%time.



%\paragraph{How running time scales with problem size}
\begin{table}[htb] 
   \centering
 % \caption{\label{t:growth} Growth in running time as problem size grows} 
   \begin{tabular}{|c|c|cccc|} \hline 
   \multicolumn{2}{|c|}{\textbf{Running time}} &  
   \multicolumn{4}{c|}{\textbf{Input size}} 
\\ \cline{1-6} 
   \emph{Function}     & \emph{Notation} & $10$ & $100$ & $1000$ & $10^7$
\\ \hline 
   Constant     & $1$      & 1  & 1  &   1 &   1 \\ \hline 
   Logarithmic  & $\lg n$ & 1  & 2& 3 & 7  \\ \hline 
   Linear       & $n$ & 1 & 10 & 100 & $10^6$   \\ \hline 
``Linearithmic" & $n  \lg n$ & 1 & 20 & 300 & $7\times 10^6$ \\ \hline 
   Quadratic    & $n^{2}$ & 1 & 100 & 10000 & $10^{12}$ \\ \hline 
   Cubic        & $n^{3}$ & 1 & 1000 & $10^6$ &  $10^{18}$ \\ \hline 
   Exponential  & $2^{n}$ & 1 & $10^{27}$ & $10^{298} $ & $10^{3010296}$  \\ \hline 
   \end{tabular}
\end{table} 
 
Note: there are about $3\times 10^{18}$ nanoseconds in a century.


\begin{Boxample}[4]
\label{exr:time-compl:2}
A quadratic algorithm with processing time \(T(n)=cn^2 \)
uses 500 elementary operations for processing $10$ data items. How many will it 
use for processing $1000$ data items? 
\end{Boxample}

\begin{Boxample}[4]
Algorithm $A$ takes $n^2$ elementary operations to sort a file of $n$ lines, while Algorithm $B$ takes $50n\lg n$. 
Which algorithm is better when $n = 10$? when $n=10^6$? How do we decide which algorithm to use?
\end{Boxample}

\begin{Boxample}[4]
\label{exr:time-compl:7A}
Algorithms \textbf{A} and \textbf{B} use exactly \(T_\mathrm{A}(n) = c_\mathrm{A} n \lg n\)
and \(T_\mathrm{B}(n) = c_\mathrm{B} n^{2}\) elementary operations, 
respectively, for a problem of size \(n\).
Find the fastest algorithm for processing \(n=2^{20}\) data items if
\textbf{A} and \textbf{B} spend 10 and 1 operations,
respectively, to process \(2^{10}\equiv 1024\) items.
\end{Boxample}


\chapter{Lecture: Techniques for estimating running time}

Here are some easy basic rules.
\begin{itemize}
\item Running time of disjoint blocks adds.
\item Running time of nested  loops with non-interacting variables multiplies.
%\item An algorithm consisting of a fixed number $k$ of nested loops going from $1$ to $n$,
%with a constant number of elementary operations each time, has running
%time of the form $C n^k$ for some constant $C$; if $k = 2$ or $k=3$, this is a 
%\alert{quadratic} or \alert{cubic} time algorithm. 
\item Example: single, double, triple loops with fixed number of elementary operations inside the inner loop yields linear, quadratic, cubic running time.
\end{itemize}




\begin{Example}
\begin{algorithm}[H]
  \caption{Swapping two elements in an array
    \label{alg:swap}}
\begin{algorithmic}[0]
\Require{$0 \leq i \leq j \leq n-1$}
\Function{swap}{array $a[0..n-1]$, integer $i$, integer $j$}
\State $t \gets a[i]$
\State $a[i] \gets a[j]$
\State $a[j] \gets t$
\State \Return{$a$}
\EndFunction
\end{algorithmic}
\end{algorithm}

This is a constant time algorithm.
\end{Example}


\begin{Example}
\begin{algorithm}[H]
  \caption{Finding the maximum in an array
    \label{alg:findmax}}
\begin{algorithmic}[0]
\Function{findmax}{array $a[0..n-1]$}
\State $k \gets 0$ \Comment{location of maximum so far}
\For {$j \gets 1$ \text{to} $n-1$} 
\If {$a[k] < a[j]$}
\State{$k = j$} 
\EndIf
\EndFor
\State \Return{$k$}
\EndFunction
\end{algorithmic}
\end{algorithm}

This is a linear time algorithm, since it makes one pass through the array and does a constant amount of work each time.
\end{Example}



\begin{Example}
\frametitle{Snippet: other loop increments}
\begin{algorithm}[H]
  \caption{Example: exponential change of variable in loop
    \label{alg:runtime1}}
\begin{algorithmic}[0]
\State \Let{$i$}{1}
\While{$i < n$}
\State $i \gets 2*i$
\State \text{print} $i$
\EndWhile
\end{algorithmic}
\end{algorithm}

This runs in logarithmic time because $i$ doubles about $\lg n$ times until reaching $n$.
\end{Example}

\begin{Example}
\begin{algorithm}[H]
  \caption{Snippet: Nested loops
    \label{alg:nestloop}}
\begin{algorithmic}[0]
\For {$i \gets 1$ \text{to} $n$} 
\For {$j \gets i$ \text{to} $n$} 
\State \text{print} $i+j$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
\end{Example}

What do we do if the control flow of the algorithm is more complicated? For example, how do we handle \textbf{if} statements?

\begin{frame}
\frametitle{Estimating running time: harder example}
\begin{tabbing}
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\= \kill
\textbf{for} ($i = 1; i < n; i\gets 2*i$) \\
\> \textbf{for} ($j = 1; j < n; j \gets 2*j$) \\
\> \> \textbf{if} ($j = 2*i$) \\
\> \> \> \textbf{for} ($k = 0; k < n; k\gets k+1$) \\
\> \> \> \> \{ constant number of operations \} \\
\> \> \textbf{else} \\
\> \> \> \textbf{for} ($k = 1; k < n; k\gets 3*k$) \\
\> \> \> \> \{ constant number of operations \}
\end{tabbing}
\pause
The $i$-loop has $l:=\lfloor \lg n \rfloor$ iterations, as does the $j$-loop. 
For each $i$ (except the last value, $i = 2^l$) the if-statement executes for 
exactly one value of $j$. The number of elementary operations is roughly 
proportional to $\lg n (n + (\lg n - 1)\log_3 n)$. 
\end{frame}



\begin{frame}
\begin{itemize}[<+->]
\item \texttt{slowfib} makes $F(n)$ function calls each of which involves a
constant number of elementary operations. It turns out that $F(n)$ grows
exponentially in $n$, so this is an \alert{exponential time algorithm}.
\end{itemize}
\end{frame}


\begin{Boxample}[6]
\label{exm:nest2}
Let us roughly estimate the running time of the following nested loops:
 
\hspace*{.3in}\begin{minipage}{5in}
\Algorbody
{
\(m \leftarrow 2\) \\
\textbf{for} \(j \leftarrow 1\) \textbf{to} \(n\) \textbf{do}\\
\>\textbf{if} \(j = m \) \textbf{then} \\
\>\> \(m \leftarrow 2m\) \\
\>\>\textbf{for} \(i \leftarrow  1\) \textbf{to} \(n\) \textbf{do}\\
\>\>\>$\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\textbf{end for} \\
\>\textbf{end if}\\
\textbf{end for}\\
}
\end{minipage}

The inner loop is executed $k$ times for $j=2, 4, \ldots, 2^{k}$
where $k < \lg n \le k+1$. The total time for the elementary operations is 
proportional to $kn$, that is, $T(n)=  n  \lfloor \lg n \rfloor$.    
\end{Boxample}

\begin{Boxample}[6]
\label{exm:nest1}
Is the running time quadratic or linear for the nested loops below?\\

\begin{minipage}{5in}
\Algorbody
{
\(m \leftarrow 1\) \\
\textbf{for} \(j \leftarrow 1\) \textbf{step} \(j \leftarrow j+1\) 
                                    \textbf{until} \(n\) \textbf{do}\\
\>\textbf{if} \(j = m \) \textbf{then} \(m \leftarrow m \cdot (n-1)\) \\
\>\>\textbf{for} \(i \leftarrow  0\) \textbf{step} \(i \leftarrow i+1\) 
                                       \textbf{until} \(n-1\) \textbf{do}\\
\>\>\>$\ldots$ \AlgCmt{constant number of elementary operations} \\
\>\>\textbf{end for} \\
\>\textbf{end if}\\
\textbf{end for} \\
}
\end{minipage}

\end{Boxample}


Conditional and switch operations like \textbf{if}
\{\emph{condition}\} \textbf{then} \{\emph{constant running time}
$T_1$\} \textbf{else} \{\emph{constant running time} $T_2$\} involve
relative frequencies of the groups of computations.  The running time
$T$ satisfies $T = f_\mathrm{true} T_1 + (1 - f_\mathrm{true}) T_2 <
\max \{ T_1, T_2 \}$ where $f_\mathrm{true}$ is the relative frequency
of the true condition value in the if-statement.

The running time of a function or method call is $T = \sum_{i=1}^{k}T_i$
where $T_i$ is the running time of statement \(i\) of the function
and $k$ is the number of statements.


\chapter{Asymptotic notation} %-----------------------------------------------

\paragraph{Asymptotic comparison of functions}
\begin{itemize}
\item In order to compare running times of algorithms we want a way of comparing
 the growth rates of functions. 
\item We want to see what happens for large values of $n$ --- small ones are not
 relevant.
\item We are not usually interested in constant factors and only want to consider the 
dominant term.
\item The standard mathematical approach is to use asymptotic notation 
$O, \Omega, \Theta$ which we will now describe.
\end{itemize}


\paragraph{Big-O notation}
\begin{Definition}

Suppose that $f$ and $g$ are functions from $\mathbb{N}$ to $\mathbb{R}$, 
which take on nonnegative values. 
\begin{itemize}
\item Say \defnfont{$f$ is $O(g)$} (``$f$ is Big-Oh of $g$") if there is
some $C > 0$ and some $n_0 \in \mathbb{N}$ such that for all $n \geq
n_0$, $f(n) \leq C g(n)$. 

Informally, $f/g$ is eventually bounded away from infinity, and $f$ grows at most as fast as $g$.

\item Say \defnfont{$f$ is $\Omega(g)$} (``$f$ is big-Omega of $g$") if $g$ is
$O(f)$. 

Informally, $f/g$ is eventually bounded away from zero, and $f$ grows at least as fast as $g$.
\item Say \defnfont{$f$ is $\Theta(g)$} (``$f$ is big-Theta of $g$") if $f$ is 
$O(g)$ and $g$ is $O(f)$. 

Informally, $f/g$ is bounded away from zero and infinity, and $f$ grows at the same rate as $g$.
\end{itemize}
\end{Definition}

Note that we could always reduce $n_0$ at the expense of a bigger $C$ but 
it is often easier not to. 

\paragraph{Asymptotic comparison --- examples}
\begin{itemize}
\item Every linear function $f(n) = an + b$, $a > 0$, is $O(n)$. Proof: 
$an + b \leq an + |b| \leq (a + |b|) n$ for $n \geq 1$. 
\item If $f(n) = n, g(n) = n^2/2$, then $f$ is $O(g)$ and $g$ is not
$O(f)$, so $g$ grows asymptotically faster than $f$. Proof: 
$f(n) \leq 2 \cdot g(n)$ for $n\geq 0$ (or $f(n) \leq 1 \cdot g(n)$ for 
$n \geq 2$); conversely, suppose 
that eventually $n^2 \leq Cn$. Then $n \leq C$ for all sufficiently large $n$, 
a contradiction.
\item $n \lg n$ is $O(2^{-10} n^2)$ since $n \lg n \leq n^2$ for $n \geq 1$.
\item We don't usually prove such results from the definition but you
need to know how to, in case the rules (coming up) don't apply.
\end{itemize}


\paragraph{Questions}
\begin{itemize}
\item What is the relationship between $n_0$ and $C$?
\item Do we have to keep proving statements of the form ``$f$ is $O(g)$" from the definition?
\item I have seen $f = O(g)$ before. How is that notation related to this?
\item What can we say about functions like $1+(-1)^n$ or $1+1/2+1/3+\dots + 1/n$?
\end{itemize}


\paragraph{Rules for asymptotic notation}
\begin{itemize}
\item (irrelevance of constant factors) if $c > 0$ is constant then $cf$ is $\Theta(f)$.
\item (transitivity) If $f$ is $O(g)/\Omega(g)/\Theta(g)$ and $g$ is 
$O(h)/\Omega(h)/\Theta(h)$, then $f$ is $O(h)/\Omega(h)/\Theta(h)$.
\item (sum rule) If $f_1$ is $O(g_1)$ and $f_2$ is $O(g_2)$ then $f_1 + f_2$ is 
$O(\max\{g_1, g_2\})$.
\item (product rule) If $f_1$ is $O(g_1)$ and $f_2$ is $O(g_2)$ then $f_1 f_2$ 
is $O(g_1 g_2)$.
\item (limit rule) \emph{Suppose that $L:=\lim_{n\to\infty} f(n)/g(n)$ exists}. 
Then
\begin{itemize}
\item if $L = 0$ then $f$ is $O(g)$ and $f$ is not $\Omega(g)$;
\item if $0 < L < \infty$ then $f$ is $\Theta(g)$;
\item if $L = \infty$ then $f$ is $\Omega(g)$ and $f$ is not $O(g)$.
\end{itemize}


L'H\^{o}pital's rule is often useful for the application of the limit rule. Note that the 
limit may not exist at all.
\end{itemize}


\paragraph{Asymptotic comparison --- more examples}
\begin{itemize}
\item $\log_a(n)$ is $\Theta(\lg n)$ for each $a > 1$, by limit rule;
\item $n \lg n$ is $O(n^2)$ and $n \lg n$ is not $\Omega(n^2)$, by limit rule;
\item $2^n$ is $\Omega(n^{100})$, by limit rule;
\item $10^{-100} n^2 + 10^{100} n$ is not $O(n)$, by limit rule;
\item $1 + (-1)^n$ is $O(1)$ but not $\Theta(1)$ since it takes on the value $0$
 infinitely often. Limit rule does not apply either.
\item Important for later: $\log (n!)$ is $\Theta(n \log n)$. 
\end{itemize}
Write $f \prec g$ if $f$ is $O(g)$ but $f$ is not $\Theta(g)$, so $g$ grows at a
 faster rate than $f$. Then, for example, 
\begin{align*}
& \log n \prec (\log n)^2 \prec \sqrt{n} \prec n \prec n \log n 
 \prec n (\log n)^2 \prec n^2 \\
& \prec n^3 \prec \dots \prec (1.5)^n \prec 2^n \prec n! \prec n^n.
\end{align*}


\paragraph{Asymptotics via integral approximation}
\begin{itemize}
\item We want the asymptotic behaviour of $\log (n!) = \sum_{k=1}^n \log k$.
\item Clearly there is an upper bound $\log n! \leq n \log n$. For a lower bound, we 
can approximate by an integral (``upper and lower rectangles") to get 
$\log n! \geq \int_1^{n+1} \log x \, dx$. Integration by parts shows that 
$\log n! \in \Omega(n \log n)$.
\item Note that the base of the logarithm is not important. More precise estimates can 
be obtained, and it turns out that $\lim_{n\to \infty} (\log n!)/(n \log n) = 1$.
\item By the same method we can get $H_n \in \Theta(\log n)$ where 
$H_n:=1+ 1/2 + 1/3 + \dots + 1/n$ is the \emph{$n$th harmonic number}. 
\end{itemize}



\chapter{Q\&A} %--------------------------------------------------------------

\paragraph{Is addition of integers an elementary operation?}
\begin{itemize}
\item If the integers (and their sum) can fit into a machine word (typically 64 bits, so not bigger than $9,223,372,036,854,775,807$) then 
adding two of them can be done in constant time.
\item If the integers are much longer, as occurs for example in symbolic algebra systems, cryptography, etc, then to avoid overflow they must be represented another way (``big integers"). Typically these are strings.
\item In this case the addition is done componentwise and the running time grows linearly with the size of the integers.
\item Assuming that $F(n)$ has the order of $n$ decimal digits (which is true), the amount of work 
done by \texttt{fastfib} is of order $1+2+\dots + n$ which is order $n^2$, not $n$. 
\item So \texttt{fastfib} is technically a quadratic time algorithm!
\end{itemize}



\paragraph{Have we been measuring input size correctly?}
\begin{itemize}
\item We used $n$ to measure the size of the integer $n$. 
\item This seems wrong. In fact the number of bits needed to represent $n$ seems a much better 
idea.
\item If $m$ is the size of the positive integer $n$, then $m = 1+\lfloor\lg n\rfloor$, and $2^{m-1} \leq n < 2^m$.
\item This turns \texttt{fastfib} from a polynomial time algorithm to an exponential time algorithm! The input size measure \emph{must} be specified.
\end{itemize}



\paragraph{What happens if there are many inputs of a given size?}
\begin{itemize}
\item We usually don't want to have to consider the distribution of
running time over all possible inputs of a given size. There may be (infinitely)
many inputs of a given size, and running time may vary widely on these. 
\item For example, for sorting the integers $1, \dots, n$, there are $n!$ possible inputs, and this is large even for $n=10$.
\item We consider statistics of $T(\inp)$ such as \defnfont{worst-case} $W(n)$ or 
\defnfont{average-case} $A(n)$ running time for instances $\inp$ of size $n$.
% Jo asks: Are W(n) and A(n) actually ever used?
\end{itemize}


\paragraph{What are the pros and cons of worst and average case analysis?}
\begin{itemize}
\item Worst-case bounds are valid for all instances: 
this is important for mission-critical applications.
\item Worst-case bounds are often easier to derive mathematically.
\item Worst-case bounds often hugely exceed typical running time and 
have little predictive or comparative value. 
\item Average-case running time is often more realistic. Quicksort is a classic 
example.
\item Average-case analysis requires a good understanding of the probability 
distribution of the inputs.
\item Conclusion: a good worst-case bound is always useful, but it is
just a first step and we should aim to refine the analysis for important 
algorithms. Average-case analysis is often more practically useful, provided the
 algorithm will be run on ``random" data. 
\end{itemize}




\paragraph{Why can constants often be ignored?}
\begin{itemize}
\item A linear time algorithm when implemented will take at most $A
n + B$ seconds to run on an instance of size $n$, for some
implementation-specific constants $A, B$. 
\item For large $n$, this is well approximated by $A n$. 
Small $n$ are not usually of interest anyway, since almost any algorithm is good
 enough for tiny instances.
\item No matter what $A$ is, we can easily work out how the running time
scales with increasing problem size (linearly!). 
\item The difference between a linear and a quadratic time algorithm is
usually huge, no matter what the constants are. For large enough $n$, a
linear time algorithm will always beat a quadratic time one. 
\item Conclusion: in practice we often need to make only crude
distinctions. We only need to know whether the running time scales like 
$n, n^2, n^3, n \log n, 2^n, \dots$. If we need finer distinctions, we 
can do more analysis.
\end{itemize}



\paragraph{Can we always ignore constants?}
\begin{itemize}
\item When we want to choose between two good algorithms for the same
problem (``is my linear-time algorithm faster than your linear-time
algorithm?"), we may need to know constants. These must be determined
empirically. 
\item For important algorithms that will be used many times, it is worth
being more precise about the constants. Even small savings will be worth
the trouble. 
\item An algorithm with running time $10^{-10} n^2$ is probably better
in practice than one with running time $10^{10} n$, since the latter
will eventually beat the former, but only on instances of size at least $10^{20}$, 
which is rarely met in practice. 
\item Conclusion: we should have at least a rough feel for the constants
of competing algorithms. However, in practice the constants are usually
of moderate size. 
\end{itemize}



\paragraph{Summary}
\begin{itemize}
\item Our goal is to find an asymptotic approximation for the (worst or average 
case) running time of a given algorithm. Ideally we can find a simple function 
$f$ and prove that the running time is $\Theta(f(n))$.
\item The main $f(n)$ occurring in applications are $\log n, n, n \log n, n^2, n^3, 2^n$, and 
each grows considerably faster than the previous one. The gap between $n$ and 
$n\log n$ is the smallest.
\end{itemize}